# Надёжность и безопасность (обязательно)

## Цель

Система должна работать автономно в транспорте:

- auto‑start после ребута
- auto‑restart при падении
- локальный буфер при потере связи
- контроль “alive” edge‑узлов

## systemd services (все узлы)

Рекомендации для unit-файлов:

- `Restart=always`
- `RestartSec=2`
- `StartLimitIntervalSec=0` (не “залипать” после серии неудачных стартов)
- лимиты (`MemoryMax=...`) по ситуации
- логирование в journald
- корректная остановка/таймауты

Практика для текущего MVP:

- `ExecStartPre` на базе `/opt/passengers-mvp/preflight.py`
- сервис стартует только когда пройдены обязательные проверки (время/доступность Central/доступность backend по VPN)

## watchdog

Минимум:

- включить аппаратный/софт watchdog (если поддерживается)
- или systemd watchdog (`WatchdogSec=`) в сервисах, которые критичны

Текущее состояние (внедрено):

- на `central-gw`, `door-1`, `door-2` активирован hardware watchdog через systemd:
  - `RuntimeWatchdogSec=30s`
  - `ShutdownWatchdogSec=2min`
- развернут сервисный watchdog:
  - `passengers-service-watchdog.service`
  - `passengers-service-watchdog.timer` (каждые `45s`)
  - скрипт `service_watchdog.py` перезапускает критичные сервисы по роли узла.

Что контролируется:

- Edge: `passengers-edge-sender.service`, `passengers-queue-maintenance.timer`
- Central: `passengers-collector.service`, `passengers-central-uplink.service`, `passengers-central-heartbeat.timer`, `wg-quick@wg0.service`, `passengers-queue-maintenance.timer`
- Central + `STOP_MODE=timer`: дополнительно `passengers-central-flush.timer`

Применение baseline:

```bash
./scripts/install_opi_watchdog.sh --opi-user orangepi --hosts '192.168.10.1 192.168.10.11 192.168.10.12' --runtime-watchdog-sec 30s --shutdown-watchdog-sec 2min --interval-sec 45
```

Проверка:

```bash
for host in 192.168.10.1 192.168.10.11 192.168.10.12; do
  ssh orangepi@$host 'systemctl is-active passengers-service-watchdog.timer; systemctl show -p RuntimeWatchdogUSec --value; sudo python3 /opt/passengers-mvp/service_watchdog.py --env /etc/passengers/passengers.env --check-only'
done
```

Последняя подтверждённая проверка (после полного power-cycle всех 3 OPi, 2026-02-08):

- `central-gw`, `door-1`, `door-2`: `systemd=running`, failed units: `0`
- `passengers-service-watchdog.timer` и `passengers-queue-maintenance.timer`: `active` на всех узлах
- resilience-тесты `12.1` и `12.2`: `PASS`
- controlled-offline `12.2` (WG down `1800s`): `PASS`, `pending 0→25→0`
- отчёты: `Docs/auto/transport-tests/edge-central-resilience-20260208-060255Z.md`, `Docs/auto/transport-tests/central-server-resilience-20260208-060336Z.md`
- отчёт длительного офлайна: `Docs/auto/transport-tests/central-server-controlled-offline-20260208-061156Z.md`
- commissioning pre-baseline (без модулей): `Docs/auto/commissioning/sys-0001-20260208-064318Z.md`
- финальный `golden baseline` для `sys-0001` выполняется только после установки модулей и камеры

## Локальный буфер

### Edge (`door-1`, `door-2`)

Если нет связи с `central-gw`:

- события пишутся локально (SQLite или flat-files)
- при восстановлении — досылаются в правильном порядке

Рекомендация: хранить уникальный ключ события `(node_id, seq)`, чтобы Central мог дедуплицировать.

### Central (`central-gw`)

Если нет интернета (Wi‑Fi/LTE):

- агрегаты/пакеты пишутся локально
- при восстановлении — досылаются на backend

В текущем MVP автоматическая отправка “остановок” идёт через:

- `STOP_MODE=timer`: `passengers-central-flush.timer` (периодический запуск `central_flush.py`)
- `STOP_MODE=manual`: flush только вручную (`central_flush.py --send-now`)

### Ретеншн и лимиты очередей (внедрено)

На всех узлах развёрнуты:

- `passengers-queue-maintenance.service`
- `passengers-queue-maintenance.timer` (каждые ~15 минут)

Логика:

- Edge (`edge.sqlite3/outbox`): лимит по возрасту и количеству
- Central (`central.sqlite3/events`, `batches_outbox[status=sent]`): лимит по возрасту и количеству
- Central (`batches_outbox[status=pending]`): лимит по количеству всегда; лимит по возрасту — опционально

Базовые параметры в `/etc/passengers/passengers.env`:

- `EDGE_OUTBOX_MAX_ROWS=50000`
- `EDGE_OUTBOX_MAX_AGE_SEC=172800`
- `CENTRAL_EVENTS_MAX_ROWS=300000`
- `CENTRAL_EVENTS_MAX_AGE_SEC=1209600`
- `CENTRAL_SENT_BATCHES_MAX_ROWS=50000`
- `CENTRAL_SENT_BATCHES_MAX_AGE_SEC=2592000`
- `CENTRAL_PENDING_BATCHES_MAX_ROWS=10000`
- `CENTRAL_PENDING_BATCHES_MAX_AGE_SEC=2592000`
- `CENTRAL_PENDING_BATCHES_DROP_AGE=0`

## Контроль “alive” edge‑узлов

Минимально:

- Central периодически проверяет доступность `door-1`/`door-2` (ping/health endpoint)
- фиксирует состояние в локальный файл/лог

Текущее состояние в MVP:

- `passengers-central-heartbeat.timer` на `central-gw` запускает `central_heartbeat.py` каждые ~45 секунд
- heartbeat отправляется в backend (`POST /api/v1/ingest/central-heartbeat`)
- payload включает:
  - статусы сервисов (`passengers-collector`, `passengers-central-uplink`, `passengers-central-flush.timer`, `wg-quick@wg0`)
  - очередь (`events_total`, `pending_batches`, `sent_batches`, `pending_oldest_age_sec`, `wg_latest_handshake_age_sec`, `stop_mode`)
  - двери (`door-1`, `door-2`: reachable + возраст последнего события)
- backend показывает это в `/admin/fleet` и `/api/admin/fleet/centrals`

Текущие правила алертов в backend (MVP):

- heartbeat age:
  - `warn` если > 90s
  - `bad` если > 240s или heartbeat невалиден
- сервисы:
  - `bad` если `passengers-collector` или `passengers-central-uplink` не `active`
  - `warn` если `wg-quick@wg0` не `active`
  - `warn` если `passengers-central-flush.timer` не `active` только при `stop_mode=timer`
- очередь:
  - `warn` если `pending_batches >= 10`
  - `bad` если `pending_batches >= 200`
  - `warn` если стареет oldest pending (`>=1800s`)
  - `bad` если oldest pending слишком старый (`>=7200s`)
- WireGuard handshake:
  - `warn` если `wg_latest_handshake_age_sec >= 300`
  - `bad` если `wg_latest_handshake_age_sec >= 900`
- двери:
  - `bad` если door unreachable
  - `warn` если нет timestamp события или `last_event_age_sec > 1800`

## Incident layer + SLA (внедрено)

Backend ведёт отдельный слой incidents (ключ `central_id + code`):

- `open` — активный алерт без ack/silence
- `acked` — активный алерт с подтверждением оператора
- `silenced` — активный алерт приглушён до `silenced_until`
- `resolved` — алерт исчез из текущего среза heartbeat/alerts

SLA для активных incidents:

- `bad` → 300 сек
- `warn` → 1800 сек
- `good` → 3600 сек

Если `incident age > SLA target`, инцидент помечается как `sla_breached=true`.

## Уведомления по incidents (внедрено)

Уведомления отправляются при событии:

- `opened` (новый активный incident)
- `escalated_bad` (эскалация severity до `bad`)
- `escalation_policy` (инцидент `open` старше `escalation_sec`)

Условие отправки: severity `bad` **или** в `code` есть `stale`.

Каналы:

- Telegram (`ALERT_NOTIFY_TELEGRAM=true`)
- Email (`ALERT_NOTIFY_EMAIL=true`)

Policy-ограничения (управляются в `/admin/fleet/notifications`):

- `mute_until` — глобальное окно тишины
- `rate_limit_sec` — минимальный интервал между отправками для одного `(central_id, code)`
- `min_severity` — нижний порог severity для отправки
- `stale_always_notify` — отправлять `*stale*` даже если severity ниже порога
- `escalation_sec` — порог для события `escalation_policy`

Лог доставки хранится в backend и доступен через:

- `GET /api/admin/fleet/incidents/notifications`
- web: `/admin/fleet/incidents` (блок `Notification delivery log`)

## Per-system API keys (внедрено)

- У каждого `system_id` отдельный ingest key.
- Управление ключами: `scripts/fleet_api_keys.py` (`ensure/rotate/revoke/sync-server`).
- Секреты локально: `fleet/secrets/system_api_keys.csv` и `fleet/secrets/admin_api_key.txt` (не коммитятся).
- После `sync-server` ключи автоматически попадают в backend `.env` (`PASSENGERS_API_KEYS`, `ADMIN_API_KEYS`).

## Бэкапы backend БД (внедрено)

На сервере:

- `passengers-db-backup.service`
- `passengers-db-backup.timer` (каждые 30 минут)
- скрипт: `/opt/passengers-backend/ops/passengers-db-backup.sh`
- артефакты: `/opt/passengers-backend/backups/passengers-*.sqlite3.gz`

Проверка восстановления:

```bash
./scripts/server_db_restore_check.sh --server-host 207.180.213.225 --server-user alis
```

## Публичная админка: минимальный hardening (внедрено)

- `HTTPS` на `8443` (self-signed до появления домена)
- `BasicAuth`
- `limit_req`/`limit_conn` в nginx
- `fail2ban` jails: `nginx-http-auth`, `nginx-limit-req`
- отдельный `ADMIN_API_KEYS` токен для `/api/admin/*` через nginx proxy
- аудит доступа: `access_log /var/log/nginx/passengers-admin.access.log`

## Firewall

Базовая политика: входящие ограничены, исходящие разрешены.

Канон по сети: `Docs/настройка ПО/План/2. Сетевые адреса и роли.md`.

## Минимальный hardening OPi (выполнено)

Сняты лишние сервисы, не нужные MVP-контуру:

- `rpcbind.service` + `rpcbind.socket` (все 3 OPi)
- `iperf3.service` (все 3 OPi)
- `smartmontools.service` (все 3 OPi)
- `dnsmasq.service` (все 3 OPi; на `central-gw` он конфликтовал с `systemd-resolved`)

Команды:

```bash
for host in 192.168.10.1 192.168.10.11 192.168.10.12; do
  ssh orangepi@$host 'sudo systemctl disable --now rpcbind.service rpcbind.socket iperf3.service smartmontools.service dnsmasq.service; sudo systemctl reset-failed'
done
```

Проверка:

```bash
ssh orangepi@192.168.10.1 'systemctl is-system-running; ss -ltnup | grep -E ":(111|5201)\\b" || echo no_111_5201'
```

## SSH

Рекомендация:

- вход по ключам
- `sudo` без пароля — опционально и только в изолированной сети

Док: `Docs/настройка ПО/План/1.3 Passwordless sudo (опционально).md`.
