# Операции (типовые действия)

## Базовая проверка “всё живо”

С ПК:

```bash
ping -c 1 192.168.10.1
ping -c 1 192.168.10.11
ping -c 1 192.168.10.12
```

SSH:

```bash
ssh orangepi@192.168.10.1 'hostnamectl --static'
ssh orangepi@192.168.10.11 'hostnamectl --static'
ssh orangepi@192.168.10.12 'hostnamectl --static'
```

## После ребута: быстрый health-check systemd

```bash
ssh orangepi@192.168.10.1 'systemctl is-system-running; systemctl --failed --no-legend'
ssh orangepi@192.168.10.11 'systemctl is-system-running; systemctl --failed --no-legend'
ssh orangepi@192.168.10.12 'systemctl is-system-running; systemctl --failed --no-legend'
```

Ожидаемо: `running` и пустой список `--failed`.

## Если `degraded` из-за `smartmontools`/`dnsmasq` (одноразовый фикс)

```bash
for host in 192.168.10.1 192.168.10.11 192.168.10.12; do
  ssh orangepi@$host 'sudo systemctl disable --now smartmontools.service dnsmasq.service; sudo systemctl reset-failed'
done
```

## Авто‑baseline (PC + Central)

```bash
python3 scripts/opizero_baseline.py --host 192.168.10.1 --user orangepi
```

Результат: `Docs/auto/baseline-summary.md`.

## Инвентаризация всех OPi (auto)

Генерирует “сырой снимок” состояния по SSH (сеть/время/порты/сервисы/nftables):

```bash
python3 scripts/opizero_inventory.py --user orangepi 192.168.10.1 192.168.10.11 192.168.10.12
```

Результат: `Docs/auto/inventory/INDEX.md`.

## Прошивка и офлайн‑настройка Door узлов

Док: `Docs/настройка ПО/0 - базовая настройка ОС/1 - Загрузка ОС на Флеш карту/2 - Быстрая прошивка SD и офлайн настройка door-1 door-2 (LAN+SSH).md`.

## `sudo` без пароля (опционально)

Док: `Docs/настройка ПО/План/1.3 Passwordless sudo (опционально).md`.

## Обновить “снимок состояния” (рекомендуется перед изменениями)

```bash
python3 scripts/opizero_inventory.py --user orangepi 192.168.10.1 192.168.10.11 192.168.10.12
```

## Backend сервер (Ubuntu + Docker Compose)

План: `Docs/настройка ПО/План/4. Backend сервер (Ubuntu + Docker Compose).md`

Проверка доступа:

```bash
ssh alis@207.180.213.225 'hostname && sudo -n true'
```

Проверка health (backend доступен **только через VPN WireGuard**):

```bash
ssh orangepi@192.168.10.1 'ping -c 2 10.66.0.1 && curl -sS http://10.66.0.1/health'
```

Управление backend (на сервере):

```bash
ssh alis@207.180.213.225 'cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml ps'
ssh alis@207.180.213.225 'cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml logs --tail=200 api'
ssh alis@207.180.213.225 'cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml restart api'
```

Проверка роли текущего admin token:

```bash
ssh alis@207.180.213.225 'token=$(grep -m1 "^ADMIN_API_KEYS=" /opt/passengers-backend/.env | cut -d= -f2 | tr -d "\r" | cut -d, -f1); curl -sS -H "Authorization: Bearer ${token}" http://10.66.0.1/api/admin/whoami'
```

Хвост admin audit log:

```bash
ssh alis@207.180.213.225 'token=$(grep -m1 "^ADMIN_API_KEYS=" /opt/passengers-backend/.env | cut -d= -f2 | tr -d "\r" | cut -d, -f1); curl -sS -H "Authorization: Bearer ${token}" "http://10.66.0.1/api/admin/audit?limit=20"'
```

Задать роли для admin token (RBAC):

```bash
ssh alis@207.180.213.225 'cd /opt/passengers-backend && sudo cp .env .env.bak && sudo sed -i "/^ADMIN_API_KEY_ROLES=/d" .env && echo "ADMIN_API_KEY_ROLES=<token_admin>:admin,<token_operator>:operator,<token_viewer>:viewer" | sudo tee -a .env >/dev/null && sudo docker compose -f compose.yaml -f compose.server.yaml up -d --build api'
```

Тест уведомлений (dry-run, без реальной отправки):

```bash
ssh alis@207.180.213.225 'token=$(grep -m1 "^ADMIN_API_KEYS=" /opt/passengers-backend/.env | cut -d= -f2 | tr -d "\r" | cut -d, -f1); curl -sS -H "Authorization: Bearer ${token}" -H "Content-Type: application/json" -d "{\"central_id\":\"central-gw\",\"code\":\"test_notification\",\"severity\":\"warn\",\"channel\":\"auto\",\"message\":\"dry run test\",\"dry_run\":true}" http://10.66.0.1/api/admin/fleet/notification-settings/test'
```

Проверка KPI history API (bucketed):

```bash
ssh alis@207.180.213.225 'token=$(grep -m1 "^ADMIN_API_KEYS=" /opt/passengers-backend/.env | cut -d= -f2 | tr -d "\r" | cut -d, -f1); curl -sS -H "Authorization: Bearer ${token}" "http://10.66.0.1/api/admin/fleet/metrics/history?window=24h&bucket_sec=300"'
```

Проверка unified monitor snapshot API (dashboard source):

```bash
ssh alis@207.180.213.225 'token=$(grep -m1 "^ADMIN_API_KEYS=" /opt/passengers-backend/.env | cut -d= -f2 | tr -d "\r" | cut -d, -f1); curl -sS -H "Authorization: Bearer ${token}" "http://10.66.0.1/api/admin/fleet/monitor?window=24h"'
```

Проверка fleet health snapshot API:

```bash
ssh alis@207.180.213.225 'token=$(grep -m1 "^ADMIN_API_KEYS=" /opt/passengers-backend/.env | cut -d= -f2 | tr -d "\r" | cut -d, -f1); curl -sS -H "Authorization: Bearer ${token}" "http://10.66.0.1/api/admin/fleet/health?window=24h"'
```

Проверка и обновление monitor policy thresholds:

```bash
ssh alis@207.180.213.225 'token=$(grep -m1 "^ADMIN_API_KEYS=" /opt/passengers-backend/.env | cut -d= -f2 | tr -d "\r" | cut -d, -f1); curl -sS -H "Authorization: Bearer ${token}" "http://10.66.0.1/api/admin/fleet/monitor-policy"'
ssh alis@207.180.213.225 'token=$(grep -m1 "^ADMIN_API_KEYS=" /opt/passengers-backend/.env | cut -d= -f2 | tr -d "\r" | cut -d, -f1); curl -sS -X POST -H "Authorization: Bearer ${token}" -H "Content-Type: application/json" -d "{\"warn_heartbeat_age_sec\":120,\"bad_heartbeat_age_sec\":600,\"warn_pending_batches\":10,\"bad_pending_batches\":200,\"warn_wg_age_sec\":300,\"bad_wg_age_sec\":1200}" "http://10.66.0.1/api/admin/fleet/monitor-policy"'
```

Per-central monitor overrides (CRUD):

```bash
ssh alis@207.180.213.225 'token=$(grep -m1 "^ADMIN_API_KEYS=" /opt/passengers-backend/.env | cut -d= -f2 | tr -d "\r" | cut -d, -f1); curl -sS -H "Authorization: Bearer ${token}" "http://10.66.0.1/api/admin/fleet/monitor-policy/overrides?limit=2000"'
ssh alis@207.180.213.225 'token=$(grep -m1 "^ADMIN_API_KEYS=" /opt/passengers-backend/.env | cut -d= -f2 | tr -d "\r" | cut -d, -f1); curl -sS -X POST -H "Authorization: Bearer ${token}" -H "Content-Type: application/json" -d "{\"central_id\":\"sys-0001\",\"warn_heartbeat_age_sec\":180,\"bad_heartbeat_age_sec\":720}" "http://10.66.0.1/api/admin/fleet/monitor-policy/overrides"'
ssh alis@207.180.213.225 'token=$(grep -m1 "^ADMIN_API_KEYS=" /opt/passengers-backend/.env | cut -d= -f2 | tr -d "\r" | cut -d, -f1); curl -sS -X DELETE -H "Authorization: Bearer ${token}" "http://10.66.0.1/api/admin/fleet/monitor-policy/overrides/sys-0001"'
```

Примечание: на масштабе `100–200` всегда используем `central_id=<SYSTEM_ID>` (например `sys-0007`), а не общий `central-gw`.

Fleet health alert test (dry-run, без реальной отправки):

```bash
ssh alis@207.180.213.225 'token=$(grep -m1 "^ADMIN_API_KEYS=" /opt/passengers-backend/.env | cut -d= -f2 | tr -d "\r" | cut -d, -f1); curl -sS -X POST -H "Authorization: Bearer ${token}" -H "Content-Type: application/json" -d "{\"channel\":\"auto\",\"window\":\"24h\",\"dry_run\":true,\"note\":\"ops smoke\"}" "http://10.66.0.1/api/admin/fleet/health/notify-test"'
```

Fleet health auto-evaluate (state-change/rate-limit):

```bash
ssh alis@207.180.213.225 'token=$(grep -m1 "^ADMIN_API_KEYS=" /opt/passengers-backend/.env | cut -d= -f2 | tr -d "\r" | cut -d, -f1); curl -sS -X POST -H "Authorization: Bearer ${token}" "http://10.66.0.1/api/admin/fleet/health/notify-auto"'
ssh alis@207.180.213.225 'token=$(grep -m1 "^ADMIN_API_KEYS=" /opt/passengers-backend/.env | cut -d= -f2 | tr -d "\r" | cut -d, -f1); curl -sS -X POST -H "Authorization: Bearer ${token}" "http://10.66.0.1/api/admin/fleet/health/notify-auto?dry_run=1&force=1"'
```

Retry конкретной записи уведомления (safe dry-run):

```bash
ssh alis@207.180.213.225 'token=$(grep -m1 "^ADMIN_API_KEYS=" /opt/passengers-backend/.env | cut -d= -f2 | tr -d "\r" | cut -d, -f1); curl -sS -H "Authorization: Bearer ${token}" -H "Content-Type: application/json" -d "{\"notification_id\":1,\"dry_run\":true}" http://10.66.0.1/api/admin/fleet/notifications/retry'
```

## MVP контур Edge→Central→Backend (без GPS/RTC)

Деплой на 3× OPi (копирование + systemd units):

```bash
./scripts/deploy_passengers_mvp.sh
```

Проверка Central collector:

```bash
curl -sS http://192.168.10.1:8080/health
```

Сгенерировать тест-событие на `door-1` и отправить на Central:

```bash
ssh orangepi@192.168.10.11 'python3 /opt/passengers-mvp/enqueue_event.py --door-id 2 --in 1 --out 0'
```

Ручной flush на backend (на Central):

```bash
ssh orangepi@192.168.10.1 'python3 /opt/passengers-mvp/central_flush.py'
```

Проверка авто-flush таймера (на Central):

```bash
ssh orangepi@192.168.10.1 'sudo sed -n "s/^STOP_MODE=//p" /etc/passengers/passengers.env | head -n1'
ssh orangepi@192.168.10.1 'systemctl is-active passengers-central-flush.timer'
ssh orangepi@192.168.10.1 'systemctl list-timers --all | grep passengers-central-flush.timer'
```

Примечание: при `STOP_MODE=manual` таймер может быть отключён (`disabled/inactive`) — это штатно.

Проверка preflight-гейта в логах:

```bash
ssh orangepi@192.168.10.1 'journalctl -u passengers-collector -n 30 --no-pager | grep preflight'
ssh orangepi@192.168.10.11 'journalctl -u passengers-edge-sender -n 30 --no-pager | grep preflight'
ssh orangepi@192.168.10.12 'journalctl -u passengers-edge-sender -n 30 --no-pager | grep preflight'
```

## Wi‑Fi интернет на Central (чтобы слать на внешний backend)

План: `Docs/настройка ПО/План/6. Интернет для central-gw (Wi‑Fi).md`

Быстрая проверка (на `central-gw`):

```bash
ssh orangepi@192.168.10.1 'ping -c 1 1.1.1.1 && ping -c 1 10.66.0.1 && curl -sS http://10.66.0.1/health'
ssh orangepi@192.168.10.1 'sudo journalctl -u passengers-central-uplink --no-pager -n 50'
```

## VPN WireGuard (рекомендуется для прод)

План: `Docs/настройка ПО/План/7. VPN WireGuard (Central↔Server).md`

Проверка с Central:

```bash
ssh orangepi@192.168.10.1 'ping -c 2 10.66.0.1 && curl -sS http://10.66.0.1/health'
```

## E2E тест (MVP): Door → Central → Backend

Цель: руками прогнать “одно событие на каждую дверь” и убедиться что:

- Door доставляет события на Central (`passengers-edge-sender` → `passengers-collector`)
- Central формирует “остановку” (manual flush) и отправляет на backend через VPN
- backend увеличивает статистику по `vehicle_id`

### 1) Сгенерировать события на door-узлах

```bash
ssh orangepi@192.168.10.11 'sudo python3 /opt/passengers-mvp/enqueue_event.py --door-id 2 --in 3 --out 1'
ssh orangepi@192.168.10.12 'sudo python3 /opt/passengers-mvp/enqueue_event.py --door-id 3 --in 1 --out 2'
```

### 2) Убедиться что Central принял события

```bash
ssh orangepi@192.168.10.1 'sudo journalctl -u passengers-collector --no-pager -n 20 | tail -n 5'
```

### 3) Сделать manual flush “остановки” на Central

```bash
ssh orangepi@192.168.10.1 'sudo python3 /opt/passengers-mvp/central_flush.py --send-now'
```

Ожидаемо: JSON с `batch_id`, HTTP 200 отправка на backend.

### 4) Проверить статистику на backend (через VPN, без раскрытия токена)

Узнать `vehicle_id` на Central:

```bash
ssh orangepi@192.168.10.1 'sudo sed -n "s/^VEHICLE_ID=//p" /etc/passengers/passengers.env | head -n1'
```

Проверка выполняется на сервере (токен берётся из `/opt/passengers-backend/.env` внутри сервера, **в консоль не выводится**):

```bash
ssh alis@207.180.213.225 'token=$(sudo sed -n "s/^PASSENGERS_API_KEYS=//p" /opt/passengers-backend/.env | head -n1 | tr -d "\r\"'\''" | cut -d, -f1); curl -sS -H "Authorization: Bearer ${token}" http://10.66.0.1/api/v1/stats/vehicle/<VEHICLE_ID>; echo'
```

### Автоматизированный smoke тест (рекомендуется)

Один запуск делает полный цикл:

- проверка сервисов на `door-1`, `door-2`, `central-gw`
- enqueue тестовых событий на двери
- `central_flush.py --send-now`
- валидация дельты статистики на backend

```bash
./scripts/mvp_e2e_smoke.sh
```

Параметры (пример):

```bash
./scripts/mvp_e2e_smoke.sh --door1-in 5 --door1-out 2 --door2-in 2 --door2-out 4 --max-wait-sec 60 --poll-sec 4
```

## Практический тест 12.2 (обрыв Central↔Server/WG)

Цель: проверить, что при потере VPN у Central batch уходит в `pending`, а после восстановления WG доставка догоняется.

```bash
./scripts/test_central_server_resilience.sh --smoke --events-per-door 2 --wait-drain-sec 120
```

Артефакты:

- `Docs/auto/transport-tests/central-server-resilience-*.md`
- `Docs/auto/transport-tests/INDEX.md`

## Масштабный dry-run реестра (100–200 систем)

Цель: до реального добавления новых комплектов проверить, что схема `system_id/vehicle_id/wg_ip` масштабируется без конфликтов.

```bash
python3 scripts/fleet_scale_dry_run.py --target-systems 200
```

Артефакты:

- `Docs/auto/fleet-scale/scale-dry-run-*.md`
- `Docs/auto/fleet-scale/registry-sim-*.csv`
- `Docs/auto/fleet-scale/INDEX.md`

## Rollout-check для `sys-0002..sys-0020` (без железа)

Цель: подтвердить, что запись в реестре, bundle и оркестратор готовы до фактического ввода нового комплекта.

```bash
python3 scripts/fleet_rollout_check.py --from 2 --to 20
```

Артефакт:

- `Docs/auto/fleet-rollout/sys-0002-rollout-check-*.md` … `sys-0020-rollout-check-*.md`
- `Docs/auto/fleet-rollout/INDEX.md`

## Пакет шаблонов `sys-0002..sys-0020` (ручной ввод)

```bash
python3 scripts/fleet_batch_template.py --from 2 --to 20
```

Артефакты:

- `fleet/templates/registry-sys-0002-0020.template.csv`
- `fleet/templates/checklist-sys-0002-0020.md`

## Per-system API keys (обязательно для масштаба)

Создать/посмотреть ключи:

```bash
python3 scripts/fleet_api_keys.py ensure --system-id sys-0001
python3 scripts/fleet_api_keys.py ensure --system-id sys-0002
python3 scripts/fleet_api_keys.py ensure --system-id sys-0003
for id in \
  sys-0004 sys-0005 sys-0006 sys-0007 sys-0008 sys-0009 sys-0010 \
  sys-0011 sys-0012 sys-0013 sys-0014 sys-0015 sys-0016 sys-0017 \
  sys-0018 sys-0019 sys-0020; do
  python3 scripts/fleet_api_keys.py ensure --system-id "$id"
done
python3 scripts/fleet_api_keys.py list
```

Ротация/отзыв:

```bash
python3 scripts/fleet_api_keys.py rotate --system-id <SYSTEM_ID>
python3 scripts/fleet_api_keys.py revoke --system-id <SYSTEM_ID> --reason <REASON>
```

Синхронизировать на сервер (backend env + nginx admin token):

```bash
python3 scripts/fleet_api_keys.py sync-server --server-host 207.180.213.225 --server-user alis
```

После sync обязательно обновить env нужного Central:

```bash
python3 scripts/fleet_apply_central_env.py --system-id <SYSTEM_ID>
```

## Бэкапы БД backend + проверка восстановления

Установить timer на сервер:

```bash
./scripts/install_server_db_backup.sh --server-host 207.180.213.225 --server-user alis
```

Проверка восстановления:

```bash
./scripts/server_db_restore_check.sh --server-host 207.180.213.225 --server-user alis
```

## Auto fleet health alert timer

Установить timer авто-алертов fleet health на сервер:

```bash
./scripts/install_server_fleet_health_auto.sh --server-host 207.180.213.225 --server-user alis
```

Проверка:

```bash
ssh alis@207.180.213.225 'sudo systemctl status passengers-fleet-health-auto.timer --no-pager'
ssh alis@207.180.213.225 'sudo journalctl -u passengers-fleet-health-auto.service -n 30 --no-pager'
```

## Hardening публичной админки

Применить nginx/fail2ban hardening:

```bash
./scripts/install_server_admin_hardening.sh --server-host 207.180.213.225 --server-user alis --admin-token-file fleet/secrets/admin_api_key.txt
```

Проверка:

```bash
ssh alis@207.180.213.225 'sudo nginx -t && sudo fail2ban-client status'
curl -kI https://207.180.213.225:8443/admin
```

## Fleet мониторинг (Central heartbeat)

Проверить heartbeat timer на Central:

```bash
ssh orangepi@192.168.10.1 'systemctl is-active passengers-central-heartbeat.timer passengers-central-heartbeat.service'
ssh orangepi@192.168.10.1 'systemctl list-timers --all | grep passengers-central-heartbeat.timer'
```

Ручной heartbeat (диагностика):

```bash
ssh orangepi@192.168.10.1 'sudo python3 /opt/passengers-mvp/central_heartbeat.py --timeout-sec 6'
```

Проверка backend API Fleet:

```bash
ssh alis@207.180.213.225 'token=$(grep -m1 "^ADMIN_API_KEYS=" /opt/passengers-backend/.env | cut -d= -f2 | tr -d "\r" | cut -d, -f1); curl -sS -H "Authorization: Bearer ${token}" http://10.66.0.1/api/admin/fleet/centrals'
```

Проверка агрегированной сводки и ленты алертов:

```bash
ssh alis@207.180.213.225 'token=$(grep -m1 "^ADMIN_API_KEYS=" /opt/passengers-backend/.env | cut -d= -f2 | tr -d "\r" | cut -d, -f1); curl -sS -H "Authorization: Bearer ${token}" "http://10.66.0.1/api/admin/fleet/overview?include_centrals=1&limit=20"'
ssh alis@207.180.213.225 'token=$(grep -m1 "^ADMIN_API_KEYS=" /opt/passengers-backend/.env | cut -d= -f2 | tr -d "\r" | cut -d, -f1); curl -sS -H "Authorization: Bearer ${token}" "http://10.66.0.1/api/admin/fleet/alerts?severity=bad&limit=50"'
ssh alis@207.180.213.225 'token=$(grep -m1 "^ADMIN_API_KEYS=" /opt/passengers-backend/.env | cut -d= -f2 | tr -d "\r" | cut -d, -f1); curl -sS -H "Authorization: Bearer ${token}" "http://10.66.0.1/api/admin/fleet/alerts?severity=warn&central_id=central-gw&code=door_events_stale&q=door&include_silenced=1&limit=50"'
ssh alis@207.180.213.225 'token=$(grep -m1 "^ADMIN_API_KEYS=" /opt/passengers-backend/.env | cut -d= -f2 | tr -d "\r" | cut -d, -f1); curl -sS -H "Authorization: Bearer ${token}" "http://10.66.0.1/api/admin/fleet/incidents?include_resolved=1&limit=50"'
ssh alis@207.180.213.225 'token=$(grep -m1 "^ADMIN_API_KEYS=" /opt/passengers-backend/.env | cut -d= -f2 | tr -d "\r" | cut -d, -f1); curl -sS -H "Authorization: Bearer ${token}" "http://10.66.0.1/api/admin/fleet/incidents/central-gw/door_events_stale?limit=30"'
ssh alis@207.180.213.225 'token=$(grep -m1 "^ADMIN_API_KEYS=" /opt/passengers-backend/.env | cut -d= -f2 | tr -d "\r" | cut -d, -f1); curl -sS -X POST -H "Authorization: Bearer ${token}" http://10.66.0.1/api/admin/fleet/incidents/sync'
ssh alis@207.180.213.225 'token=$(grep -m1 "^ADMIN_API_KEYS=" /opt/passengers-backend/.env | cut -d= -f2 | tr -d "\r" | cut -d, -f1); curl -sS -H "Authorization: Bearer ${token}" "http://10.66.0.1/api/admin/fleet/incidents/notifications?channel=telegram&status=failed&limit=50"'
ssh alis@207.180.213.225 'token=$(grep -m1 "^ADMIN_API_KEYS=" /opt/passengers-backend/.env | cut -d= -f2 | tr -d "\r" | cut -d, -f1); curl -sS -H "Authorization: Bearer ${token}" "http://10.66.0.1/api/admin/fleet/notification-settings"'
ssh alis@207.180.213.225 'token=$(grep -m1 "^ADMIN_API_KEYS=" /opt/passengers-backend/.env | cut -d= -f2 | tr -d "\r" | cut -d, -f1); curl -sS -X POST -H "Authorization: Bearer ${token}" -H "Content-Type: application/json" -d "{\"min_severity\":\"bad\",\"rate_limit_sec\":300,\"escalation_sec\":1800}" "http://10.66.0.1/api/admin/fleet/notification-settings"'
ssh alis@207.180.213.225 'token=$(grep -m1 "^ADMIN_API_KEYS=" /opt/passengers-backend/.env | cut -d= -f2 | tr -d "\r" | cut -d, -f1); curl -sS -H "Authorization: Bearer ${token}" "http://10.66.0.1/api/admin/fleet/central/central-gw?limit=50"'
ssh alis@207.180.213.225 'token=$(grep -m1 "^ADMIN_API_KEYS=" /opt/passengers-backend/.env | cut -d= -f2 | tr -d "\r" | cut -d, -f1); curl -sS -H "Authorization: Bearer ${token}" "http://10.66.0.1/api/admin/fleet/alerts/actions?central_id=central-gw&limit=50"'
ssh alis@207.180.213.225 'token=$(grep -m1 "^ADMIN_API_KEYS=" /opt/passengers-backend/.env | cut -d= -f2 | tr -d "\r" | cut -d, -f1); curl -sS -H "Authorization: Bearer ${token}" "http://10.66.0.1/api/admin/fleet/alerts/actions?action=silence&q=maintenance&limit=50"'
```

Сводка по алертам (быстрый чек):

```bash
ssh alis@207.180.213.225 'token=$(grep -m1 "^ADMIN_API_KEYS=" /opt/passengers-backend/.env | cut -d= -f2 | tr -d "\r" | cut -d, -f1); curl -sS -H "Authorization: Bearer ${token}" http://10.66.0.1/api/admin/fleet/centrals | python3 -c "import json,sys; d=json.load(sys.stdin); c=(d.get(\"centrals\") or [{}])[0]; h=c.get(\"health\") or {}; print(\"severity=\",h.get(\"severity\"),\"alerts=\",h.get(\"alerts_total\"),\"warn=\",h.get(\"alerts_warn\"),\"bad=\",h.get(\"alerts_bad\"))"'
```

Web‑панель Fleet:

```bash
# VPN-only
http://10.66.0.1/admin/fleet
http://10.66.0.1/admin/fleet/incidents
http://10.66.0.1/admin/fleet/incidents/central-gw/door_events_stale
http://10.66.0.1/admin/fleet/notifications
http://10.66.0.1/admin/fleet/central/central-gw
http://10.66.0.1/admin/fleet/actions

# публичный доступ через nginx + BasicAuth
https://207.180.213.225:8443/admin/fleet
https://207.180.213.225:8443/admin/fleet/incidents
https://207.180.213.225:8443/admin/fleet/incidents/central-gw/door_events_stale
https://207.180.213.225:8443/admin/fleet/notifications
https://207.180.213.225:8443/admin/fleet/central/central-gw
https://207.180.213.225:8443/admin/fleet/actions
```

На странице Fleet:

- фильтр `только alert` показывает только проблемные Centrals
- фильтр `severity` (`all/good/warn/bad`) отбирает Centrals по уровню
- фильтр `include silenced` показывает/скрывает приглушённые алерты
- фильтры `alert central_id` и `alert code` уточняют ленту алертов по ключам
- в колонке `Alerts` отображаются коды и тексты причин
- блок `Alert stream` показывает общий топ алертов по fleet
- блок `Оперативний пріоритет` на `/admin/fleet` показывает ключевые риски: `критичні вузли`, `WG stale`, `вузли з чергою`, `SLA порушено`
- быстрые фокус-кнопки на `/admin/fleet` (`критичні`, `WG stale`, `черга`, `door stale`, `скинути`) автоматически выставляют фильтры
- индикатор `Фільтри: ...` на `/admin/fleet` показывает активный режим фильтрации и уровень фокуса (`good/warn/bad`)
- на странице `Central detail` есть `Admin action log` (кто/когда ack/silence/unsilence)
- страница `Fleet action log` показывает журнал действий по всем Central с фильтрами `central_id/code/action/q`
- страница `Fleet incidents` показывает слой инцидентов (статусы, SLA breach) и лог отправки уведомлений
- на странице `/admin/fleet/incidents` добавлены быстрые пресеты фильтров (`відкриті`, `критичні`, `SLA порушення`, `заглушені`, `скинути`)
- на странице `/admin/fleet/incidents` индикатор `Фільтри: ...` показывает активную комбинацию фильтров и её критичность
- страница `Incident detail` показывает timeline конкретного инцидента и быстрые action-кнопки
- страница `Notification rules` управляет `mute / rate-limit / escalation` без редактирования backend кода
- UI админки переведён на украинский (основные экраны `/admin`, `/admin/fleet*`, `/admin/wg`, `/admin/audit`)
- в интерфейсе унифицированы термины: `Вузол` (вместо `Central`) и `Деталі` (вместо `Drill-down`)
- в `Fleet incidents` добавлены `Теплокарта SLA` и массовые действия (`ack/silence/unsilence`) с учётом RBAC (`operator+`)

Действия по алертам (через API):

```bash
# ACK
ssh alis@207.180.213.225 'token=$(grep -m1 "^ADMIN_API_KEYS=" /opt/passengers-backend/.env | cut -d= -f2 | tr -d "\r" | cut -d, -f1); curl -sS -X POST -H "Content-Type: application/json" -H "Authorization: Bearer ${token}" -d "{\"central_id\":\"central-gw\",\"code\":\"door_events_stale\",\"actor\":\"ops\",\"note\":\"manual ack\"}" http://10.66.0.1/api/admin/fleet/alerts/ack'

# Silence на 1 час
ssh alis@207.180.213.225 'token=$(grep -m1 "^ADMIN_API_KEYS=" /opt/passengers-backend/.env | cut -d= -f2 | tr -d "\r" | cut -d, -f1); curl -sS -X POST -H "Content-Type: application/json" -H "Authorization: Bearer ${token}" -d "{\"central_id\":\"central-gw\",\"code\":\"door_events_stale\",\"duration_sec\":3600,\"actor\":\"ops\",\"note\":\"maintenance window\"}" http://10.66.0.1/api/admin/fleet/alerts/silence'

# Unsilence
ssh alis@207.180.213.225 'token=$(grep -m1 "^ADMIN_API_KEYS=" /opt/passengers-backend/.env | cut -d= -f2 | tr -d "\r" | cut -d, -f1); curl -sS -X POST -H "Content-Type: application/json" -H "Authorization: Bearer ${token}" -d "{\"central_id\":\"central-gw\",\"code\":\"door_events_stale\",\"actor\":\"ops\",\"note\":\"window closed\"}" http://10.66.0.1/api/admin/fleet/alerts/unsilence'
```

Транспортные секреты уведомлений backend (`/opt/passengers-backend/.env`):

```bash
# включить каналы
ALERT_NOTIFY_TELEGRAM=true
ALERT_NOTIFY_EMAIL=false

# Telegram
ALERT_TELEGRAM_BOT_TOKEN=<bot_token>
ALERT_TELEGRAM_CHAT_ID=<chat_id>

# Email (опционально)
ALERT_EMAIL_FROM=alerts@example.com
ALERT_EMAIL_TO=ops@example.com
ALERT_SMTP_HOST=smtp.example.com
ALERT_SMTP_PORT=587
ALERT_SMTP_USER=<smtp_user>
ALERT_SMTP_PASS=<smtp_pass>
ALERT_SMTP_STARTTLS=true
```

Policy-правила (вкл/выкл каналов, `min_severity`, `mute_until`, `rate_limit_sec`, `escalation_sec`) задаются через API/UI:

- `GET/POST /api/admin/fleet/notification-settings`
- web: `/admin/fleet/notifications`

## Отложенный этап: реальные каналы алертов + аварийный E2E

Этап не выполняем сейчас; запускаем после выдачи production-секретов Telegram/SMTP.

Шаги:

1) На сервере задать секреты каналов в `/opt/passengers-backend/.env` (`ALERT_TELEGRAM_*`, `ALERT_SMTP_*`) и включить `ALERT_NOTIFY_TELEGRAM`/`ALERT_NOTIFY_EMAIL`.
2) Перезапустить backend:

```bash
ssh alis@207.180.213.225 'cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml up -d --build'
```

3) В UI `/admin/fleet/notifications` задать рабочие policy-параметры (`min_severity`, `rate_limit_sec`, `escalation_sec`, `stale_always_notify`).
4) Прогнать аварийный E2E:
   - создать контролируемый bad/warn инцидент;
   - запустить `POST /api/admin/fleet/health/notify-auto`;
   - проверить доставку в `notify-center` и реальный канал;
   - восстановить состояние и проверить recovery-поведение.
5) Зафиксировать отчёт: `Docs/auto/alerts-e2e/<timestamp>.md`.

Критерий готовности:

- alert реально доставляется в канал и отражается в `notify-center`;
- после recovery уведомления не фладят (работают `mute/rate-limit/escalation`).

## Деплой backend (важно: server override)

```bash
ssh alis@207.180.213.225 'cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml up -d --build'
ssh orangepi@192.168.10.1 'curl -sS http://10.66.0.1/health'
```

## Массовый rollout (100–200 систем)

Реестр:

```bash
python3 scripts/fleet_registry.py validate
python3 scripts/fleet_registry.py next-wg-ip
```

Сгенерировать bundle для системы:

```bash
python3 scripts/fleet_registry.py bundle --system-id <SYSTEM_ID>
```

Применить WG peer на сервере (idempotent):

```bash
python3 scripts/fleet_apply_wg_peer.py --system-id <SYSTEM_ID> --fetch-central-pubkey
```

Параметризованный деплой MVP:

```bash
source fleet/out/<SYSTEM_ID>/fleet.env
./scripts/deploy_passengers_mvp.sh \
  --central-ip "${CENTRAL_IP}" \
  --edge-ips "${EDGE_IPS_CSV}" \
  --user "${OPI_USER}" \
  --server-host "${SERVER_HOST}" \
  --server-user "${SERVER_SSH_USER}" \
  --backend-host "${BACKEND_HOST}" \
  --stop-mode "${STOP_MODE}" \
  --stop-flush-interval-sec "${STOP_FLUSH_INTERVAL_SEC}"
```

Применить env template на central:

```bash
python3 scripts/fleet_apply_central_env.py --system-id <SYSTEM_ID>
```

Быстрый оркестратор:

```bash
./scripts/fleet_rollout.sh --system-id <SYSTEM_ID> --all-safe
./scripts/fleet_rollout.sh --system-id <SYSTEM_ID> --apply-wg-peer --wg-fetch-central-pubkey --all-safe
./scripts/fleet_rollout.sh --system-id <SYSTEM_ID> --apply-wg-peer --wg-fetch-central-pubkey --wg-ensure-central-key --all-safe
```

Commissioning-отчёт для системы (рекомендуется после rollout):

```bash
python3 scripts/fleet_commission.py --system-id <SYSTEM_ID> --smoke
```

Результаты:

- `Docs/auto/commissioning/<SYSTEM_ID>-<timestamp>.md`
- `Docs/auto/commissioning/INDEX.md`

## Путь проработки связи (стандартный)

Перед подключением новых модулей (камера/GPS/RTC/LTE) всегда идём по одной цепочке:

1) `Edge → Central` (LAN, доставка событий и буферизация)
2) `Central → Server` (WireGuard, heartbeat, batch delivery)
3) `End-to-End` (`mvp_e2e_smoke.sh`)
4) `Commissioning` (`fleet_commission.py`) и фиксация отчёта

Референс этапов:

- `Docs/настройка ПО/План/12. Связь Edge-Central-Server (этап внедрения модулей).md`

Текущая активная зона: только `12.1` и `12.2` (без камер и без модулей).

### Практический тест 12.1 (обрыв Edge→Central + догонка)

```bash
./scripts/test_edge_central_resilience.sh --smoke --events-per-door 4 --wait-drain-sec 120
```

Артефакт:

- `Docs/auto/transport-tests/edge-central-resilience-<timestamp>.md`

### Практический тест 12.2 (обрыв Central→Server + догонка)

```bash
./scripts/test_central_server_resilience.sh --events-per-door 2 --wait-drain-sec 180
```

Артефакт:

- `Docs/auto/transport-tests/central-server-resilience-<timestamp>.md`

## Шаг 1 baseline (OPi без модулей): watchdog + reboot resilience

Применить watchdog baseline на все 3 OPi:

```bash
./scripts/install_opi_watchdog.sh --opi-user orangepi --hosts '192.168.10.1 192.168.10.11 192.168.10.12' --runtime-watchdog-sec 30s --shutdown-watchdog-sec 2min --interval-sec 45
```

Проверка watchdog:

```bash
for host in 192.168.10.1 192.168.10.11 192.168.10.12; do
  ssh orangepi@$host 'systemctl is-active passengers-service-watchdog.timer; systemctl show -p RuntimeWatchdogUSec --value; sudo python3 /opt/passengers-mvp/service_watchdog.py --env /etc/passengers/passengers.env --check-only'
done
```

Smoke после baseline:

```bash
./scripts/mvp_e2e_smoke.sh --central-ip 192.168.10.1 --door1-ip 192.168.10.11 --door2-ip 192.168.10.12 --server-host 207.180.213.225 --max-wait-sec 60 --poll-sec 3
```

Reboot-resilience (по одному узлу):

```bash
ssh orangepi@192.168.10.11 'sudo reboot'
ssh orangepi@192.168.10.12 'sudo reboot'
ssh orangepi@192.168.10.1 'sudo reboot'
```

После каждого ребута проверять:

```bash
for host in 192.168.10.1 192.168.10.11 192.168.10.12; do
  ssh orangepi@$host 'systemctl is-system-running || true; systemctl is-active passengers-service-watchdog.timer; systemctl is-active passengers-queue-maintenance.timer; systemctl is-active passengers-edge-sender.service 2>/dev/null || true; systemctl is-active passengers-collector.service 2>/dev/null || true; systemctl is-active passengers-central-uplink.service 2>/dev/null || true'
done
```

### Факт проверки `sys-0001` после power-cycle (2026-02-08)

Проверено после отключения/включения питания всех 3 OPi:

- `central-gw`: boot `2026-02-07 22:15:37`, `systemd=running`, failed units: `0`
- `door-1`: boot `2026-02-07 22:14:12`, `systemd=running`, failed units: `0`
- `door-2`: boot `2026-02-07 22:15:03`, `systemd=running`, failed units: `0`
- На `central-gw` активны `passengers-collector`, `passengers-central-uplink`, `wg-quick@wg0`, watchdog/maintenance timers
- `passengers-central-flush.timer` может быть `disabled/inactive` при `STOP_MODE=manual` — это штатно
- Backend API на VPS жив: `http://10.66.0.1/health -> {"status":"ok"}`

Фактические resilience прогоны (последовательно, без параллели):

```bash
./scripts/test_edge_central_resilience.sh --smoke --events-per-door 4 --wait-drain-sec 120 --poll-sec 2
./scripts/test_central_server_resilience.sh --smoke --events-per-door 2 --wait-drain-sec 150 --poll-sec 2
```

Результат: оба теста `PASS`.

Артефакты:

- `Docs/auto/transport-tests/edge-central-resilience-20260208-060255Z.md`
- `Docs/auto/transport-tests/central-server-resilience-20260208-060336Z.md`
- `Docs/auto/transport-tests/INDEX.md`

## Шаг 2 (без модулей): профиль длительного офлайна

Обновить central env из bundle (включает лимиты `pending` очереди):

```bash
python3 scripts/fleet_registry.py bundle --system-id sys-0001
python3 scripts/fleet_apply_central_env.py --system-id sys-0001
```

Проверить применённые параметры на Central:

```bash
ssh orangepi@192.168.10.1 "sudo awk -F= '/^(CENTRAL_PENDING_BATCHES_MAX_ROWS|CENTRAL_PENDING_BATCHES_MAX_AGE_SEC|CENTRAL_PENDING_BATCHES_DROP_AGE|CENTRAL_EVENTS_MAX_ROWS|CENTRAL_SENT_BATCHES_MAX_ROWS|STOP_MODE)=/{print}' /etc/passengers/passengers.env"
ssh orangepi@192.168.10.1 'sudo python3 /opt/passengers-mvp/queue_maintainer.py --mode auto --env /etc/passengers/passengers.env --central-db /var/lib/passengers/central.sqlite3'
```

Установить monitor policy под длительный офлайн:

```bash
curl -k -u "admin:<BASIC_AUTH_PASS>" -H "Content-Type: application/json" \
  -d '{"warn_heartbeat_age_sec":120,"bad_heartbeat_age_sec":600,"warn_pending_batches":10,"bad_pending_batches":200,"warn_wg_age_sec":300,"bad_wg_age_sec":1200}' \
  https://207.180.213.225:8443/api/admin/fleet/monitor-policy
```

Валидация после шага 2:

```bash
SERVER_USER=alis ./scripts/mvp_e2e_smoke.sh --central-ip 192.168.10.1 --door1-ip 192.168.10.11 --door2-ip 192.168.10.12 --server-host 207.180.213.225 --max-wait-sec 60 --poll-sec 3
./scripts/test_edge_central_resilience.sh --events-per-door 2 --wait-drain-sec 90 --poll-sec 2
./scripts/test_central_server_resilience.sh --events-per-door 1 --wait-drain-sec 120 --poll-sec 2
```

### Шаг 2.1 (обязательно): controlled-offline 30 минут

Цель: проверить длительный офлайн Central→Server и гарантированную догонку после восстановления WG.

```bash
./scripts/test_central_server_controlled_offline.sh --outage-sec 1800 --cycle-sec 60 --events-per-cycle 1 --wait-drain-sec 900 --poll-sec 5 --smoke
```

Результат последнего прогона (`2026-02-08`): `PASS`.

Артефакты:

- `Docs/auto/transport-tests/central-server-controlled-offline-20260208-061156Z.md`
- `Docs/auto/transport-tests/INDEX.md`

### Шаг 3: зафиксировать pre-baseline `sys-0001` (без модулей)

Сформировать commissioning-report после шагов 1/2/2.1:

```bash
python3 scripts/fleet_commission.py --system-id sys-0001 --smoke
```

Последний отчёт (`2026-02-08`):

- `Docs/auto/commissioning/sys-0001-20260208-064318Z.md`
- `Docs/auto/commissioning/INDEX.md`

Решение команды (`2026-02-08`): финальный `golden baseline` для `sys-0001` откладывается до установки модулей и камеры.

Текущий статус:

- `sys-0001` = `pre-baseline` (этап без модулей/камеры).
- Финальная эталонная фиксация выполняется после интеграции OAK-D Lite и остальных модулей.

Критерий готовности `sys-0001` к финальному статусу `golden baseline`:

- `fail=0` и `warn=0` в commissioning;
- `systemd=running` на `central-gw`, `door-1`, `door-2`;
- `pending=0` после controlled-offline;
- WG handshake и Fleet severity `good`.

## Fleet: оперативная лента событий (`ops-feed`)

Проверка API (из VPN/сервера):

```bash
ssh alis@207.180.213.225 'token=$(grep -m1 "^ADMIN_API_KEYS=" /opt/passengers-backend/.env | cut -d= -f2 | tr -d "\r" | cut -d, -f1); curl -sS -H "Authorization: Bearer ${token}" "http://10.66.0.1/api/admin/fleet/ops-feed?window=24h&limit=80&include_resolved=0&include_silenced=0"'
```

Фильтрация по central/code/severity:

```bash
ssh alis@207.180.213.225 'token=$(grep -m1 "^ADMIN_API_KEYS=" /opt/passengers-backend/.env | cut -d= -f2 | tr -d "\r" | cut -d, -f1); curl -sS -H "Authorization: Bearer ${token}" "http://10.66.0.1/api/admin/fleet/ops-feed?window=24h&central_id=sys-0001&code=wg_stale&severity=bad&limit=50"'
```

Проверка UI:

- открыть `https://207.180.213.225:8443/admin/fleet`;
- убедиться, что видны блоки `Оперативний пріоритет` и `Оперативна стрічка подій`;
- убедиться, что кнопки фокуса (`критичні/WG/черга/door stale`) переключают фильтры без ошибок в браузере.

## Alerts Ops: оперативный triage алертов

Новая страница операций:

- `https://207.180.213.225:8443/admin/fleet/alerts`

Проверка grouped API:

```bash
ssh alis@207.180.213.225 'token=$(grep -m1 "^ADMIN_API_KEYS=" /opt/passengers-backend/.env | cut -d= -f2 | tr -d "\r" | cut -d, -f1); curl -sS -H "Authorization: Bearer ${token}" "http://10.66.0.1/api/admin/fleet/alerts/groups?limit=50"'
```

Проверка страницы (наличие group table и bulk actions):

```bash
ssh alis@207.180.213.225 'curl -k -u "admin:<BASIC_AUTH_PASS>" -sS https://127.0.0.1:8443/admin/fleet/alerts | grep -nE "grpTbl|bulkAck|alerts/groups"'
```

## Модульный этап админки: `notify-center` вынесен из `main.py`

Локальная валидация:

```bash
python3 -m py_compile backend/app/main.py backend/app/admin_fleet_notifications_page.py backend/app/admin_fleet_notify_center_page.py
python3 -m compileall -q backend/app
```

Выкатка на VPS:

```bash
scp backend/app/main.py backend/app/admin_fleet_notifications_page.py backend/app/admin_fleet_notify_center_page.py alis@207.180.213.225:/tmp/
ssh alis@207.180.213.225 'sudo install -m 644 /tmp/main.py /opt/passengers-backend/app/main.py && sudo install -m 644 /tmp/admin_fleet_notifications_page.py /opt/passengers-backend/app/admin_fleet_notifications_page.py && sudo install -m 644 /tmp/admin_fleet_notify_center_page.py /opt/passengers-backend/app/admin_fleet_notify_center_page.py && cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml up -d --build api'
```

Smoke матрица:

```bash
ssh alis@207.180.213.225 'for url in \
https://127.0.0.1:8443/admin/fleet/notifications \
https://127.0.0.1:8443/admin/fleet/notify-center \
https://127.0.0.1:8443/api/admin/fleet/notification-settings; do curl -k -u "admin:<BASIC_AUTH_PASS>" -s -o /dev/null -w "$url => %{http_code}\n" "$url"; done'
ssh alis@207.180.213.225 'sudo docker logs --tail 120 passengers-backend-api-1'
```

## Модульный этап админки: `incidents` вынесен из `main.py`

Локальная валидация:

```bash
python3 -m py_compile backend/app/main.py backend/app/admin_fleet_incidents_page.py backend/app/admin_fleet_incident_detail_page.py
python3 -m compileall -q backend/app
```

Выкатка на VPS:

```bash
scp backend/app/main.py backend/app/admin_fleet_incidents_page.py backend/app/admin_fleet_incident_detail_page.py alis@207.180.213.225:/tmp/
ssh alis@207.180.213.225 'sudo install -m 644 /tmp/main.py /opt/passengers-backend/app/main.py && sudo install -m 644 /tmp/admin_fleet_incidents_page.py /opt/passengers-backend/app/admin_fleet_incidents_page.py && sudo install -m 644 /tmp/admin_fleet_incident_detail_page.py /opt/passengers-backend/app/admin_fleet_incident_detail_page.py && cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml up -d --build api'
```

Smoke матрица:

```bash
ssh alis@207.180.213.225 'for url in \
https://127.0.0.1:8443/admin/fleet/incidents \
https://127.0.0.1:8443/admin/fleet/incidents/sys-0001/wg_stale \
https://127.0.0.1:8443/api/admin/fleet/incidents \
https://127.0.0.1:8443/api/admin/fleet/incidents/notifications; do curl -k -u "admin:<BASIC_AUTH_PASS>" -s -o /dev/null -w "$url => %{http_code}\n" "$url"; done'
ssh alis@207.180.213.225 'sudo docker logs --tail 120 passengers-backend-api-1 | grep -E "ResponseValidationError|Traceback|coroutine object" || true'
```

## Модульный этап админки: `actions` и `audit` вынесены из `main.py`

Локальная валидация:

```bash
python3 -m py_compile backend/app/main.py backend/app/admin_fleet_actions_page.py backend/app/admin_audit_page.py
python3 -m compileall -q backend/app
```

Выкатка на VPS:

```bash
scp backend/app/main.py backend/app/admin_fleet_actions_page.py backend/app/admin_audit_page.py alis@207.180.213.225:/tmp/
ssh alis@207.180.213.225 'sudo install -m 644 /tmp/main.py /opt/passengers-backend/app/main.py && sudo install -m 644 /tmp/admin_fleet_actions_page.py /opt/passengers-backend/app/admin_fleet_actions_page.py && sudo install -m 644 /tmp/admin_audit_page.py /opt/passengers-backend/app/admin_audit_page.py && cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml up -d --build api'
```

Smoke матрица:

```bash
ssh alis@207.180.213.225 'for url in \
https://127.0.0.1:8443/admin/fleet/actions \
https://127.0.0.1:8443/admin/audit \
https://127.0.0.1:8443/api/admin/fleet/alerts/actions \
https://127.0.0.1:8443/api/admin/audit; do curl -k -u "admin:<BASIC_AUTH_PASS>" -s -o /dev/null -w "$url => %{http_code}\n" "$url"; done'
ssh alis@207.180.213.225 'sudo docker logs --tail 120 passengers-backend-api-1 | grep -E "ResponseValidationError|Traceback|coroutine object" || true'
```

## Единый smoke-gate админки (phase-9)

Быстрый единый прогон всех основных UI/API маршрутов админки + проверка логов API:

```bash
./scripts/admin_panel_smoke_gate.sh --server-host 207.180.213.225 --server-user alis --admin-user admin --admin-pass "<BASIC_AUTH_PASS>"
```

Поведение:

- проверяет матрицу `/admin/...` и `/api/admin/...` (ожидается `200`);
- сканирует `docker logs passengers-backend-api-1` на `ResponseValidationError|Traceback|coroutine object`;
- возвращает код `0` только если весь gate пройден.

## Модульный этап API: `incidents/actions/audit` вынесены в `*_ops.py` (phase-10)

Локальная валидация:

```bash
python3 -m py_compile backend/app/main.py backend/app/admin_incidents_ops.py backend/app/admin_audit_ops.py
python3 -m compileall -q backend/app
```

Выкатка на VPS:

```bash
scp backend/app/main.py backend/app/admin_incidents_ops.py backend/app/admin_audit_ops.py alis@207.180.213.225:/tmp/
ssh alis@207.180.213.225 'sudo install -m 644 /tmp/main.py /opt/passengers-backend/app/main.py && sudo install -m 644 /tmp/admin_incidents_ops.py /opt/passengers-backend/app/admin_incidents_ops.py && sudo install -m 644 /tmp/admin_audit_ops.py /opt/passengers-backend/app/admin_audit_ops.py && cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml up -d --build api'
```

Smoke (API + UI):

```bash
ssh alis@207.180.213.225 'for url in \
https://127.0.0.1:8443/api/admin/fleet/incidents \
https://127.0.0.1:8443/api/admin/fleet/incidents/notifications \
https://127.0.0.1:8443/api/admin/fleet/alerts/actions \
https://127.0.0.1:8443/api/admin/audit \
https://127.0.0.1:8443/admin/fleet/incidents \
https://127.0.0.1:8443/admin/fleet/actions \
https://127.0.0.1:8443/admin/audit; do curl -k -u "admin:<BASIC_AUTH_PASS>" -s -o /dev/null -w "$url => %{http_code}\n" "$url"; done'
```

## Модульный этап API: `notification-settings` + `monitor-policy` вынесены в `*_ops.py` (phase-11)

Локальная валидация:

```bash
python3 -m py_compile backend/app/main.py backend/app/admin_notification_ops.py backend/app/admin_monitor_policy_ops.py
python3 -m compileall -q backend/app
```

Выкатка на VPS:

```bash
scp backend/app/main.py backend/app/admin_notification_ops.py backend/app/admin_monitor_policy_ops.py alis@207.180.213.225:/tmp/
ssh alis@207.180.213.225 'sudo install -m 644 /tmp/main.py /opt/passengers-backend/app/main.py && sudo install -m 644 /tmp/admin_notification_ops.py /opt/passengers-backend/app/admin_notification_ops.py && sudo install -m 644 /tmp/admin_monitor_policy_ops.py /opt/passengers-backend/app/admin_monitor_policy_ops.py && cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml up -d --build api'
```

Smoke (GET + safe POST):

```bash
ssh alis@207.180.213.225 'for url in \
https://127.0.0.1:8443/api/admin/fleet/monitor-policy \
https://127.0.0.1:8443/api/admin/fleet/monitor-policy/overrides \
https://127.0.0.1:8443/api/admin/fleet/notification-settings; do curl -k -u "admin:<BASIC_AUTH_PASS>" -s -o /dev/null -w "$url => %{http_code}\n" "$url"; done'
ssh alis@207.180.213.225 'curl -k -u "admin:<BASIC_AUTH_PASS>" -sS -H "Content-Type: application/json" -d "{\"central_id\":\"central-gw\",\"code\":\"test_notification\",\"severity\":\"warn\",\"channel\":\"auto\",\"message\":\"phase11 smoke\",\"dry_run\":true}" https://127.0.0.1:8443/api/admin/fleet/notification-settings/test'
```

## Модульный этап runtime/config: composition-only `main.py` (phase-12)

Локальная валидация:

```bash
python3 -m py_compile backend/app/main.py backend/app/admin_notification_ops.py backend/app/admin_monitor_policy_ops.py backend/app/admin_runtime_config.py backend/app/admin_incidents_ops.py backend/app/admin_audit_ops.py
python3 -m compileall -q backend/app
```

Выкатка на VPS:

```bash
scp backend/app/main.py backend/app/admin_notification_ops.py backend/app/admin_monitor_policy_ops.py backend/app/admin_runtime_config.py alis@207.180.213.225:/tmp/
ssh alis@207.180.213.225 'sudo install -m 644 /tmp/main.py /opt/passengers-backend/app/main.py && sudo install -m 644 /tmp/admin_notification_ops.py /opt/passengers-backend/app/admin_notification_ops.py && sudo install -m 644 /tmp/admin_monitor_policy_ops.py /opt/passengers-backend/app/admin_monitor_policy_ops.py && sudo install -m 644 /tmp/admin_runtime_config.py /opt/passengers-backend/app/admin_runtime_config.py && cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml up -d --build api'
```

Smoke и логи:

```bash
./scripts/admin_panel_smoke_gate.sh --server-host 207.180.213.225 --server-user alis --admin-user admin --admin-pass "<BASIC_AUTH_PASS>"
ssh alis@207.180.213.225 'token=$(grep -m1 "^ADMIN_API_KEYS=" /opt/passengers-backend/.env | cut -d= -f2 | tr -d "\r" | cut -d, -f1); for u in "http://10.66.0.1/api/admin/fleet/monitor-policy" "http://10.66.0.1/api/admin/fleet/monitor-policy/overrides?limit=20" "http://10.66.0.1/api/admin/fleet/notification-settings"; do curl -sS -H "Authorization: Bearer ${token}" -o /dev/null -w "$u => %{http_code}\n" "$u"; done'
ssh alis@207.180.213.225 'token=$(grep -m1 "^ADMIN_API_KEYS=" /opt/passengers-backend/.env | cut -d= -f2 | tr -d "\r" | cut -d, -f1); curl -sS -H "Authorization: Bearer ${token}" -H "Content-Type: application/json" -d "{\"central_id\":\"central-gw\",\"code\":\"test_notification\",\"severity\":\"warn\",\"channel\":\"auto\",\"message\":\"phase12 smoke\",\"dry_run\":true}" http://10.66.0.1/api/admin/fleet/notification-settings/test'
ssh alis@207.180.213.225 'token=$(grep -m1 "^ADMIN_API_KEYS=" /opt/passengers-backend/.env | cut -d= -f2 | tr -d "\r" | cut -d, -f1); curl -sS -H "Authorization: Bearer ${token}" -H "Content-Type: application/json" -d "{\"notification_id\":1,\"dry_run\":true}" http://10.66.0.1/api/admin/fleet/notifications/retry'
ssh alis@207.180.213.225 'sudo docker logs --tail 200 passengers-backend-api-1 2>&1 | grep -En "ResponseValidationError|Traceback|coroutine object|ERROR" || true'
```

## Модульный этап UI/UX: UA-copy + debounce filters (phase-13)

Локальная валидация:

```bash
python3 -m py_compile backend/app/admin_fleet_page.py backend/app/admin_fleet_alerts_page.py backend/app/admin_fleet_incidents_page.py backend/app/admin_fleet_incident_detail_page.py
python3 -m compileall -q backend/app
```

Выкатка на VPS:

```bash
scp backend/app/admin_fleet_page.py backend/app/admin_fleet_alerts_page.py backend/app/admin_fleet_incidents_page.py backend/app/admin_fleet_incident_detail_page.py alis@207.180.213.225:/tmp/
ssh alis@207.180.213.225 'sudo install -m 644 /tmp/admin_fleet_page.py /opt/passengers-backend/app/admin_fleet_page.py && sudo install -m 644 /tmp/admin_fleet_alerts_page.py /opt/passengers-backend/app/admin_fleet_alerts_page.py && sudo install -m 644 /tmp/admin_fleet_incidents_page.py /opt/passengers-backend/app/admin_fleet_incidents_page.py && sudo install -m 644 /tmp/admin_fleet_incident_detail_page.py /opt/passengers-backend/app/admin_fleet_incident_detail_page.py && cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml up -d --build api'
```

Smoke и проверка UI копирайта:

```bash
./scripts/admin_panel_smoke_gate.sh --server-host 207.180.213.225 --server-user alis --admin-user admin --admin-pass "<BASIC_AUTH_PASS>"
ssh alis@207.180.213.225 'for url in /admin/fleet /admin/fleet/alerts /admin/fleet/incidents /admin/fleet/incidents/central-gw/door_events_stale; do echo "== $url =="; curl -k -u "admin:<BASIC_AUTH_PASS>" -sS "https://127.0.0.1:8443${url}" | grep -E "Стрічка 24г|Скинути фільтри|Підтвердити|Пауза 1 год" -m 3 || true; done'
ssh alis@207.180.213.225 'sudo docker logs --tail 200 passengers-backend-api-1 2>&1 | grep -En "ResponseValidationError|Traceback|coroutine object|ERROR" || true'
```

## Модульный этап UI toolkit: общая оболочка страниц (phase-14A)

Локальная валидация:

```bash
python3 -m py_compile backend/app/admin_ui_kit.py backend/app/admin_audit_page.py backend/app/admin_fleet_actions_page.py backend/app/admin_fleet_history_page.py backend/app/main.py
python3 -m compileall -q backend/app
```

Выкатка на VPS:

```bash
scp backend/app/admin_ui_kit.py backend/app/admin_audit_page.py backend/app/admin_fleet_actions_page.py backend/app/admin_fleet_history_page.py alis@207.180.213.225:/tmp/
ssh alis@207.180.213.225 'sudo install -m 644 /tmp/admin_ui_kit.py /opt/passengers-backend/app/admin_ui_kit.py && sudo install -m 644 /tmp/admin_audit_page.py /opt/passengers-backend/app/admin_audit_page.py && sudo install -m 644 /tmp/admin_fleet_actions_page.py /opt/passengers-backend/app/admin_fleet_actions_page.py && sudo install -m 644 /tmp/admin_fleet_history_page.py /opt/passengers-backend/app/admin_fleet_history_page.py && cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml up -d --build api'
```

Smoke и UI-check:

```bash
./scripts/admin_panel_smoke_gate.sh --server-host 207.180.213.225 --server-user alis --admin-user admin --admin-pass "<BASIC_AUTH_PASS>"
ssh alis@207.180.213.225 'for url in /admin/audit /admin/fleet/actions /admin/fleet/history; do echo "== $url =="; curl -k -u "admin:<BASIC_AUTH_PASS>" -sS "https://127.0.0.1:8443${url}" | grep -E "Скинути фільтри|Журнал|Історія KPI|центр сповіщень" -m 4 || true; done'
ssh alis@207.180.213.225 'sudo docker logs --tail 200 passengers-backend-api-1 2>&1 | grep -En "ResponseValidationError|Traceback|coroutine object|ERROR" || true'
```

## Модульный этап UI toolkit: policy/notifications/notify-center (phase-14B)

Локальная валидация:

```bash
python3 -m py_compile backend/app/admin_ui_kit.py backend/app/admin_fleet_policy_page.py backend/app/admin_fleet_notifications_page.py backend/app/admin_fleet_notify_center_page.py backend/app/main.py
python3 -m compileall -q backend/app
```

Выкатка на VPS:

```bash
scp backend/app/admin_ui_kit.py backend/app/admin_fleet_policy_page.py backend/app/admin_fleet_notifications_page.py backend/app/admin_fleet_notify_center_page.py alis@207.180.213.225:/tmp/
ssh alis@207.180.213.225 'sudo install -m 644 /tmp/admin_ui_kit.py /opt/passengers-backend/app/admin_ui_kit.py && sudo install -m 644 /tmp/admin_fleet_policy_page.py /opt/passengers-backend/app/admin_fleet_policy_page.py && sudo install -m 644 /tmp/admin_fleet_notifications_page.py /opt/passengers-backend/app/admin_fleet_notifications_page.py && sudo install -m 644 /tmp/admin_fleet_notify_center_page.py /opt/passengers-backend/app/admin_fleet_notify_center_page.py && cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml up -d --build api'
```

Smoke и UI-check:

```bash
./scripts/admin_panel_smoke_gate.sh --server-host 207.180.213.225 --server-user alis --admin-user admin --admin-pass "<BASIC_AUTH_PASS>"
ssh alis@207.180.213.225 'for url in /admin/fleet/policy /admin/fleet/notifications /admin/fleet/notify-center; do echo "== $url =="; curl -k -u "admin:<BASIC_AUTH_PASS>" -sS "https://127.0.0.1:8443${url}" | grep -E "Політика моніторингу|Правила сповіщень|Центр сповіщень|Скинути фільтри" -m 5 || true; done'
ssh alis@207.180.213.225 'sudo docker logs --tail 200 passengers-backend-api-1 2>&1 | grep -En "ResponseValidationError|Traceback|coroutine object|ERROR" || true'
```

## Модульный этап UI toolkit: общий JS helper и унификация страниц (phase-15)

Локальная валидация:

```bash
python3 -m py_compile backend/app/admin_ui_kit.py backend/app/admin_audit_page.py backend/app/admin_fleet_actions_page.py backend/app/admin_fleet_history_page.py backend/app/admin_fleet_policy_page.py backend/app/admin_fleet_notifications_page.py backend/app/admin_fleet_notify_center_page.py backend/app/main.py
python3 -m compileall -q backend/app
```

Выкатка на VPS:

```bash
scp backend/app/admin_ui_kit.py backend/app/admin_audit_page.py backend/app/admin_fleet_actions_page.py backend/app/admin_fleet_history_page.py backend/app/admin_fleet_policy_page.py backend/app/admin_fleet_notifications_page.py backend/app/admin_fleet_notify_center_page.py alis@207.180.213.225:/tmp/
ssh alis@207.180.213.225 'sudo install -m 644 /tmp/admin_ui_kit.py /opt/passengers-backend/app/admin_ui_kit.py && sudo install -m 644 /tmp/admin_audit_page.py /opt/passengers-backend/app/admin_audit_page.py && sudo install -m 644 /tmp/admin_fleet_actions_page.py /opt/passengers-backend/app/admin_fleet_actions_page.py && sudo install -m 644 /tmp/admin_fleet_history_page.py /opt/passengers-backend/app/admin_fleet_history_page.py && sudo install -m 644 /tmp/admin_fleet_policy_page.py /opt/passengers-backend/app/admin_fleet_policy_page.py && sudo install -m 644 /tmp/admin_fleet_notifications_page.py /opt/passengers-backend/app/admin_fleet_notifications_page.py && sudo install -m 644 /tmp/admin_fleet_notify_center_page.py /opt/passengers-backend/app/admin_fleet_notify_center_page.py && cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml up -d --build api'
```

Smoke и проверка наличия toolkit helper:

```bash
./scripts/admin_panel_smoke_gate.sh --server-host 207.180.213.225 --server-user alis --admin-user admin --admin-pass "<BASIC_AUTH_PASS>"
for url in /admin/audit /admin/fleet/actions /admin/fleet/history /admin/fleet/policy /admin/fleet/notifications /admin/fleet/notify-center; do echo "== ${url} =="; curl -k -u "admin:<BASIC_AUTH_PASS>" -sS "https://207.180.213.225:8443${url}" | grep -c "window.AdminUiKit"; done
ssh alis@207.180.213.225 'sudo docker logs --tail 200 passengers-backend-api-1 2>&1 | grep -En "ResponseValidationError|Traceback|coroutine object|ERROR" || true'
```

## Модульный этап UI toolkit: heavy-страницы (`fleet/alerts/incidents/incident-detail`) (phase-16)

Локальная валидация:

```bash
python3 -m py_compile backend/app/admin_ui_kit.py backend/app/admin_fleet_page.py backend/app/admin_fleet_alerts_page.py backend/app/admin_fleet_incidents_page.py backend/app/admin_fleet_incident_detail_page.py backend/app/main.py
python3 -m compileall -q backend/app
```

Выкатка на VPS:

```bash
scp backend/app/admin_fleet_page.py backend/app/admin_fleet_alerts_page.py backend/app/admin_fleet_incidents_page.py backend/app/admin_fleet_incident_detail_page.py alis@207.180.213.225:/tmp/
ssh alis@207.180.213.225 'sudo install -m 644 /tmp/admin_fleet_page.py /opt/passengers-backend/app/admin_fleet_page.py && sudo install -m 644 /tmp/admin_fleet_alerts_page.py /opt/passengers-backend/app/admin_fleet_alerts_page.py && sudo install -m 644 /tmp/admin_fleet_incidents_page.py /opt/passengers-backend/app/admin_fleet_incidents_page.py && sudo install -m 644 /tmp/admin_fleet_incident_detail_page.py /opt/passengers-backend/app/admin_fleet_incident_detail_page.py && cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml up -d --build api'
```

Smoke и проверка подключения `AdminUiKit`:

```bash
./scripts/admin_panel_smoke_gate.sh --server-host 207.180.213.225 --server-user alis --admin-user admin --admin-pass "<BASIC_AUTH_PASS>"
for url in /admin/fleet /admin/fleet/alerts /admin/fleet/incidents /admin/fleet/incidents/central-gw/door_events_stale; do echo "== ${url} =="; curl -k -u "admin:<BASIC_AUTH_PASS>" -sS "https://207.180.213.225:8443${url}" | grep -c "window.AdminUiKit"; done
ssh alis@207.180.213.225 'sudo docker logs --tail 200 passengers-backend-api-1 2>&1 | grep -En "ResponseValidationError|Traceback|coroutine object|ERROR" || true'
```

## Модульный этап shell-wrapper: `render_admin_shell` для legacy heavy-страниц (phase-17)

Локальная валидация:

```bash
python3 -m py_compile backend/app/admin_ui_kit.py backend/app/admin_fleet_page.py backend/app/admin_fleet_alerts_page.py backend/app/admin_fleet_incidents_page.py backend/app/admin_fleet_incident_detail_page.py backend/app/main.py
python3 -m compileall -q backend/app
```

Выкатка на VPS:

```bash
scp backend/app/admin_ui_kit.py backend/app/admin_fleet_page.py backend/app/admin_fleet_alerts_page.py backend/app/admin_fleet_incidents_page.py backend/app/admin_fleet_incident_detail_page.py alis@207.180.213.225:/tmp/
ssh alis@207.180.213.225 'sudo install -m 644 /tmp/admin_ui_kit.py /opt/passengers-backend/app/admin_ui_kit.py && sudo install -m 644 /tmp/admin_fleet_page.py /opt/passengers-backend/app/admin_fleet_page.py && sudo install -m 644 /tmp/admin_fleet_alerts_page.py /opt/passengers-backend/app/admin_fleet_alerts_page.py && sudo install -m 644 /tmp/admin_fleet_incidents_page.py /opt/passengers-backend/app/admin_fleet_incidents_page.py && sudo install -m 644 /tmp/admin_fleet_incident_detail_page.py /opt/passengers-backend/app/admin_fleet_incident_detail_page.py && cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml up -d --build api'
```

Smoke и проверки:

```bash
./scripts/admin_panel_smoke_gate.sh --server-host 207.180.213.225 --server-user alis --admin-user admin --admin-pass "<BASIC_AUTH_PASS>"
for url in /admin/fleet /admin/fleet/alerts /admin/fleet/incidents /admin/fleet/incidents/central-gw/door_events_stale; do echo "== ${url} =="; curl -k -u "admin:<BASIC_AUTH_PASS>" -sS "https://207.180.213.225:8443${url}" | grep -c "window.AdminUiKit"; done
ssh alis@207.180.213.225 'sudo docker logs --tail 200 passengers-backend-api-1 2>&1 | grep -En "ResponseValidationError|Traceback|coroutine object|ERROR" || true'
```

## Модульный этап shell-wrapper: auto extraction в `render_legacy_admin_page` (phase-18A)

Локальная валидация:

```bash
python3 -m py_compile backend/app/admin_ui_kit.py backend/app/main.py
python3 -m compileall -q backend/app
```

Выкатка на VPS:

```bash
scp backend/app/admin_ui_kit.py alis@207.180.213.225:/tmp/
ssh alis@207.180.213.225 'sudo install -m 644 /tmp/admin_ui_kit.py /opt/passengers-backend/app/admin_ui_kit.py && cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml up -d --build api'
```

Smoke и HTML-check:

```bash
./scripts/admin_panel_smoke_gate.sh --server-host 207.180.213.225 --server-user alis --admin-user admin --admin-pass "<BASIC_AUTH_PASS>"
for url in /admin/fleet /admin/fleet/alerts /admin/fleet/incidents /admin/fleet/incidents/central-gw/door_events_stale; do echo "== ${url} =="; curl -k -u "admin:<BASIC_AUTH_PASS>" -sS "https://207.180.213.225:8443${url}" | grep -E -c "<header>|<div class=\"title\">"; done
ssh alis@207.180.213.225 'sudo docker logs --tail 200 passengers-backend-api-1 2>&1 | grep -En "ResponseValidationError|Traceback|coroutine object|ERROR" || true'
```

## Модульный этап чистого shell-render: incident-detail (phase-18B step-1)

Локальная валидация:

```bash
python3 -m py_compile backend/app/admin_fleet_incident_detail_page.py backend/app/admin_ui_kit.py backend/app/main.py
python3 -m compileall -q backend/app
```

Выкатка на VPS:

```bash
scp backend/app/admin_ui_kit.py backend/app/admin_fleet_incident_detail_page.py alis@207.180.213.225:/tmp/
ssh alis@207.180.213.225 'sudo install -m 644 /tmp/admin_ui_kit.py /opt/passengers-backend/app/admin_ui_kit.py && sudo install -m 644 /tmp/admin_fleet_incident_detail_page.py /opt/passengers-backend/app/admin_fleet_incident_detail_page.py && cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml up -d --build api'
```

Smoke и detail-check:

```bash
./scripts/admin_panel_smoke_gate.sh --server-host 207.180.213.225 --server-user alis --admin-user admin --admin-pass "<BASIC_AUTH_PASS>"
curl -k -u "admin:<BASIC_AUTH_PASS>" -sS "https://207.180.213.225:8443/admin/fleet/incidents/central-gw/door_events_stale" | grep -E -c "<header>|<div class=\"title\">|window.AdminUiKit"
ssh alis@207.180.213.225 'sudo docker logs --tail 120 passengers-backend-api-1 2>&1 | grep -En "ResponseValidationError|Traceback|coroutine object|ERROR" || true'
```

## Модульный этап чистого shell-render: incidents list (phase-18B step-2)

Локальная валидация:

```bash
python3 -m py_compile backend/app/admin_fleet_incidents_page.py backend/app/admin_ui_kit.py backend/app/main.py
python3 -m compileall -q backend/app
```

Выкатка на VPS:

```bash
scp backend/app/admin_fleet_incidents_page.py alis@207.180.213.225:/tmp/
ssh alis@207.180.213.225 'sudo install -m 644 /tmp/admin_fleet_incidents_page.py /opt/passengers-backend/app/admin_fleet_incidents_page.py && cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml up -d --build api'
```

Smoke и incidents-check:

```bash
./scripts/admin_panel_smoke_gate.sh --server-host 207.180.213.225 --server-user alis --admin-user admin --admin-pass "<BASIC_AUTH_PASS>"
curl -k -u "admin:<BASIC_AUTH_PASS>" -sS "https://207.180.213.225:8443/admin/fleet/incidents" | grep -E -c "<header>|<div class=\"title\">|window.AdminUiKit"
ssh alis@207.180.213.225 'sudo docker logs --tail 160 passengers-backend-api-1 2>&1 | grep -En "ResponseValidationError|Traceback|coroutine object|ERROR" || true'
```

## Модульный этап чистого shell-render: alerts (phase-18B step-3)

Локальная валидация:

```bash
python3 -m py_compile backend/app/admin_fleet_alerts_page.py backend/app/admin_fleet_incidents_page.py backend/app/main.py
python3 -m compileall -q backend/app
```

Выкатка на VPS:

```bash
scp backend/app/admin_fleet_alerts_page.py alis@207.180.213.225:/tmp/
ssh alis@207.180.213.225 'sudo install -m 644 /tmp/admin_fleet_alerts_page.py /opt/passengers-backend/app/admin_fleet_alerts_page.py && cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml up -d --build api'
```

Smoke и alerts-check:

```bash
./scripts/admin_panel_smoke_gate.sh --server-host 207.180.213.225 --server-user alis --admin-user admin --admin-pass "<BASIC_AUTH_PASS>"
curl -k -u "admin:<BASIC_AUTH_PASS>" -sS "https://207.180.213.225:8443/admin/fleet/alerts" | grep -E -c "<header>|<div class=\"title\">|window.AdminUiKit"
ssh alis@207.180.213.225 'sudo docker logs --tail 160 passengers-backend-api-1 2>&1 | grep -En "ResponseValidationError|Traceback|coroutine object|ERROR" || true'
```

## Модульный этап чистого shell-render: fleet (phase-18B step-4)

Локальная валидация:

```bash
python3 -m py_compile backend/app/admin_fleet_page.py backend/app/admin_fleet_alerts_page.py backend/app/admin_fleet_incidents_page.py backend/app/main.py
python3 -m compileall -q backend/app
```

Выкатка на VPS:

```bash
scp backend/app/admin_fleet_page.py alis@207.180.213.225:/tmp/
ssh alis@207.180.213.225 'sudo install -m 644 /tmp/admin_fleet_page.py /opt/passengers-backend/app/admin_fleet_page.py && cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml up -d --build api'
```

Smoke и fleet-check:

```bash
./scripts/admin_panel_smoke_gate.sh --server-host 207.180.213.225 --server-user alis --admin-user admin --admin-pass "<BASIC_AUTH_PASS>"
for url in /admin/fleet /admin/fleet/alerts /admin/fleet/incidents; do echo "== ${url} =="; curl -k -u "admin:<BASIC_AUTH_PASS>" -sS "https://207.180.213.225:8443${url}" | grep -E -c "<header>|<div class=\"title\">|window.AdminUiKit"; done
ssh alis@207.180.213.225 'sudo docker logs --tail 180 passengers-backend-api-1 2>&1 | grep -En "ResponseValidationError|Traceback|coroutine object|ERROR" || true'
```

## Модульный этап cleanup/hardening: post-migration (phase-19)

Локальная валидация:

```bash
python3 -m py_compile backend/app/admin_ui_kit.py backend/app/admin_fleet_page.py backend/app/admin_fleet_alerts_page.py backend/app/admin_fleet_incidents_page.py backend/app/admin_fleet_incident_detail_page.py backend/app/main.py
python3 -m compileall -q backend/app
bash -n scripts/admin_panel_smoke_gate.sh
```

Выкатка на VPS:

```bash
scp backend/app/admin_ui_kit.py backend/app/admin_fleet_page.py backend/app/admin_fleet_alerts_page.py backend/app/admin_fleet_incidents_page.py alis@207.180.213.225:/tmp/
ssh alis@207.180.213.225 'sudo install -m 644 /tmp/admin_ui_kit.py /opt/passengers-backend/app/admin_ui_kit.py && sudo install -m 644 /tmp/admin_fleet_page.py /opt/passengers-backend/app/admin_fleet_page.py && sudo install -m 644 /tmp/admin_fleet_alerts_page.py /opt/passengers-backend/app/admin_fleet_alerts_page.py && sudo install -m 644 /tmp/admin_fleet_incidents_page.py /opt/passengers-backend/app/admin_fleet_incidents_page.py && cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml up -d --build api'
```

Smoke и legacy-guard:

```bash
./scripts/admin_panel_smoke_gate.sh --server-host 207.180.213.225 --server-user alis --admin-user admin --admin-pass "<BASIC_AUTH_PASS>"
for url in /admin/fleet /admin/fleet/alerts /admin/fleet/incidents; do echo "== ${url} =="; curl -k -u "admin:<BASIC_AUTH_PASS>" -sS "https://207.180.213.225:8443${url}" | grep -E -c "<header>|<div class=\"title\">|window.AdminUiKit"; done
ssh alis@207.180.213.225 'sudo docker logs --tail 200 passengers-backend-api-1 2>&1 | grep -En "ResponseValidationError|Traceback|coroutine object|ERROR" || true'
```

Примечание:

```bash
# если нужно временно отключить модульный guard на локальной машине:
./scripts/admin_panel_smoke_gate.sh --strict-modules 0 --admin-pass "<BASIC_AUTH_PASS>"
```

## Модульный этап UI/UX polish (UA): phase-20

Локальная валидация:

```bash
python3 -m py_compile backend/app/admin_ui_kit.py backend/app/admin_fleet_page.py backend/app/admin_fleet_alerts_page.py backend/app/admin_fleet_incidents_page.py backend/app/main.py
python3 -m compileall -q backend/app
bash -n scripts/admin_panel_smoke_gate.sh
```

Выкатка на VPS:

```bash
scp backend/app/admin_ui_kit.py backend/app/admin_fleet_page.py backend/app/admin_fleet_alerts_page.py backend/app/admin_fleet_incidents_page.py alis@207.180.213.225:/tmp/
ssh alis@207.180.213.225 'sudo install -m 644 /tmp/admin_ui_kit.py /opt/passengers-backend/app/admin_ui_kit.py && sudo install -m 644 /tmp/admin_fleet_page.py /opt/passengers-backend/app/admin_fleet_page.py && sudo install -m 644 /tmp/admin_fleet_alerts_page.py /opt/passengers-backend/app/admin_fleet_alerts_page.py && sudo install -m 644 /tmp/admin_fleet_incidents_page.py /opt/passengers-backend/app/admin_fleet_incidents_page.py && cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml up -d --build api'
```

Smoke и UX-check:

```bash
./scripts/admin_panel_smoke_gate.sh --server-host 207.180.213.225 --server-user alis --admin-user admin --admin-pass "<BASIC_AUTH_PASS>"
for url in /admin/fleet /admin/fleet/alerts /admin/fleet/incidents; do echo "== ${url} =="; curl -k -u "admin:<BASIC_AUTH_PASS>" -sS "https://207.180.213.225:8443${url}" | grep -E -c "<header>|<div class=\"title\">|window.AdminUiKit"; done
ssh alis@207.180.213.225 'sudo docker logs --tail 220 passengers-backend-api-1 2>&1 | grep -En "ResponseValidationError|Traceback|coroutine object|ERROR" || true'
```

## Модульный этап operator workflow/navigation: phase-21

Локальная валидация:

```bash
python3 -m py_compile backend/app/admin_ui_kit.py backend/app/admin_fleet_page.py backend/app/admin_fleet_alerts_page.py backend/app/admin_fleet_incidents_page.py backend/app/main.py
python3 -m compileall -q backend/app
bash -n scripts/admin_panel_smoke_gate.sh
```

Выкатка на VPS:

```bash
scp backend/app/admin_ui_kit.py backend/app/admin_fleet_page.py backend/app/admin_fleet_alerts_page.py backend/app/admin_fleet_incidents_page.py alis@207.180.213.225:/tmp/
ssh alis@207.180.213.225 'sudo install -m 644 /tmp/admin_ui_kit.py /opt/passengers-backend/app/admin_ui_kit.py && sudo install -m 644 /tmp/admin_fleet_page.py /opt/passengers-backend/app/admin_fleet_page.py && sudo install -m 644 /tmp/admin_fleet_alerts_page.py /opt/passengers-backend/app/admin_fleet_alerts_page.py && sudo install -m 644 /tmp/admin_fleet_incidents_page.py /opt/passengers-backend/app/admin_fleet_incidents_page.py && cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml up -d --build api'
```

Smoke и operator-flow checks:

```bash
./scripts/admin_panel_smoke_gate.sh --server-host 207.180.213.225 --server-user alis --admin-user admin --admin-pass "<BASIC_AUTH_PASS>"
for url in /admin/fleet /admin/fleet/alerts /admin/fleet/incidents /admin/audit; do echo "== ${url} =="; curl -k -u "admin:<BASIC_AUTH_PASS>" -sS "https://207.180.213.225:8443${url}" | grep -E -c "<header>|<div class=\"title\">|window.AdminUiKit|flowStep"; done
ssh alis@207.180.213.225 'sudo docker logs --tail 240 passengers-backend-api-1 2>&1 | grep -En "ResponseValidationError|Traceback|coroutine object|ERROR" || true'
```

## Модульный этап operator UX rhythm: phase-22

Локальная валидация:

```bash
python3 -m py_compile backend/app/admin_ui_kit.py backend/app/admin_fleet_page.py backend/app/admin_fleet_alerts_page.py backend/app/admin_fleet_incidents_page.py backend/app/main.py
python3 -m compileall -q backend/app
bash -n scripts/admin_panel_smoke_gate.sh
```

Выкатка на VPS:

```bash
rsync -av backend/app/admin_ui_kit.py backend/app/admin_fleet_page.py backend/app/admin_fleet_alerts_page.py backend/app/admin_fleet_incidents_page.py alis@207.180.213.225:/opt/passengers-backend/app/
ssh alis@207.180.213.225 'cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml up -d --build api'
```

Smoke и density/meta checks:

```bash
./scripts/admin_panel_smoke_gate.sh --server-host 207.180.213.225 --server-user alis --admin-user admin --admin-pass "<BASIC_AUTH_PASS>"
for url in /admin/fleet /admin/fleet/alerts /admin/fleet/incidents /admin/audit; do echo "== ${url} =="; curl -k -u "admin:<BASIC_AUTH_PASS>" -sS "https://207.180.213.225:8443${url}" | grep -E -c "window.AdminUiKit|tableMeta|metaChip|densityBadge"; done
ssh alis@207.180.213.225 'sudo docker logs --tail 260 passengers-backend-api-1 2>&1 | grep -En "ResponseValidationError|Traceback|coroutine object|ERROR" || true'
```

## Модульный этап mission-control escalation routing: phase-44

Локальная валидация:

```bash
python3 -m py_compile backend/app/admin_ui_kit.py backend/app/admin_fleet_page.py backend/app/admin_fleet_alerts_page.py backend/app/admin_fleet_incidents_page.py backend/app/main.py
python3 -m compileall -q backend/app
bash -n scripts/admin_panel_smoke_gate.sh
```

Выкатка на VPS:

```bash
rsync -av backend/app/admin_ui_kit.py alis@207.180.213.225:/opt/passengers-backend/app/
ssh alis@207.180.213.225 'cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml up -d --build api'
```

Smoke и routing-marker checks:

```bash
./scripts/admin_panel_smoke_gate.sh --server-host 207.180.213.225 --server-user alis --admin-user admin --admin-pass "<BASIC_AUTH_PASS>"
for url in /admin/fleet /admin/fleet/alerts /admin/fleet/incidents /admin/audit; do
  echo "== ${url} =="
  curl -k -u "admin:<BASIC_AUTH_PASS>" -sS "https://207.180.213.225:8443${url}" | grep -E -c "sideRoutingProfile|sideRoutingTemplate|sideResponseRoutingSummary|buildMissionRoutingSignal|resolveMissionRoutingProfile|buildMissionChannelTemplate|runMissionResponsePackAction"
done
ssh alis@207.180.213.225 'sudo docker logs --tail 220 passengers-backend-api-1 2>&1 | grep -En "ResponseValidationError|Traceback|coroutine object|ERROR" || true'
```

## Модульный этап mission-control delivery adapters: phase-45

Локальная валидация:

```bash
python3 -m py_compile backend/app/admin_ui_kit.py backend/app/admin_fleet_page.py backend/app/admin_fleet_alerts_page.py backend/app/admin_fleet_incidents_page.py backend/app/main.py
python3 -m compileall -q backend/app
bash -n scripts/admin_panel_smoke_gate.sh
```

Выкатка на VPS:

```bash
rsync -av backend/app/admin_ui_kit.py alis@207.180.213.225:/opt/passengers-backend/app/
ssh alis@207.180.213.225 'cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml up -d --build api'
```

Smoke и delivery-marker checks:

```bash
./scripts/admin_panel_smoke_gate.sh --server-host 207.180.213.225 --server-user alis --admin-user admin --admin-pass "<BASIC_AUTH_PASS>"
for url in /admin/fleet /admin/fleet/alerts /admin/fleet/incidents /admin/audit; do
  echo "== ${url} =="
  curl -k -u "admin:<BASIC_AUTH_PASS>" -sS "https://207.180.213.225:8443${url}" | grep -E -c "sideDeliveryAdapter|sideDeliveryVariant|sideDeliverySummary|MISSION_DELIVERY_ADAPTER_STORAGE_KEY|buildMissionDeliveryPayload|buildMissionDeliveryTemplates|copy-telegram|copy-email|copy-ticket|copy-delivery|apply-delivery"
done
ssh alis@207.180.213.225 'sudo docker logs --tail 220 passengers-backend-api-1 2>&1 | grep -En "ResponseValidationError|Traceback|coroutine object|ERROR" || true'
```

## Модульный этап mission-control delivery state handoff: phase-46

Локальная валидация:

```bash
python3 -m py_compile backend/app/admin_ui_kit.py backend/app/admin_fleet_page.py backend/app/admin_fleet_alerts_page.py backend/app/admin_fleet_incidents_page.py backend/app/main.py
python3 -m compileall -q backend/app
bash -n scripts/admin_panel_smoke_gate.sh
```

Выкатка на VPS:

```bash
rsync -av backend/app/admin_ui_kit.py alis@207.180.213.225:/opt/passengers-backend/app/
ssh alis@207.180.213.225 'cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml up -d --build api'
```

Smoke и delivery-handoff marker checks:

```bash
./scripts/admin_panel_smoke_gate.sh --server-host 207.180.213.225 --server-user alis --admin-user admin --admin-pass "<BASIC_AUTH_PASS>"
for url in /admin/fleet /admin/fleet/alerts /admin/fleet/incidents /admin/audit; do
  echo "== ${url} =="
  curl -k -u "admin:<BASIC_AUTH_PASS>" -sS "https://207.180.213.225:8443${url}" | grep -E -c "MISSION_DELIVERY_JOURNAL_STORAGE_KEY|listMissionDeliveryJournal|renderMissionDeliveryHandoffStatus|show-delivery-journal|clear-delivery-journal|sideDeliveryHandoffStatus"
done
ssh alis@207.180.213.225 'sudo docker logs --tail 260 passengers-backend-api-1 2>&1 | grep -En "ResponseValidationError|Traceback|coroutine object|ERROR" || true'
```

## Модульный этап mission-control delivery acknowledgment loop: phase-47

Локальная валидация:

```bash
python3 -m py_compile backend/app/admin_ui_kit.py backend/app/admin_fleet_page.py backend/app/admin_fleet_alerts_page.py backend/app/admin_fleet_incidents_page.py backend/app/main.py
python3 -m compileall -q backend/app
bash -n scripts/admin_panel_smoke_gate.sh
```

Выкатка на VPS:

```bash
rsync -av backend/app/admin_ui_kit.py alis@207.180.213.225:/opt/passengers-backend/app/
ssh alis@207.180.213.225 'cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml up -d --build api'
```

Smoke и delivery-ack marker checks:

```bash
./scripts/admin_panel_smoke_gate.sh --server-host 207.180.213.225 --server-user alis --admin-user admin --admin-pass "<BASIC_AUTH_PASS>"
for url in /admin/fleet /admin/fleet/alerts /admin/fleet/incidents /admin/audit; do
  echo "== ${url} =="
  curl -k -u "admin:<BASIC_AUTH_PASS>" -sS "https://207.180.213.225:8443${url}" | grep -E -c "ack-delivery|retry-delivery|escalate-delivery|sideDeliverySlaStatus|buildMissionDeliverySlaState|renderMissionDeliverySlaStatus|missionDeliveryJournalActionLabel"
done
ssh alis@207.180.213.225 'sudo docker logs --tail 260 passengers-backend-api-1 2>&1 | grep -En "ResponseValidationError|Traceback|coroutine object|ERROR" || true'
```

## Модульный этап mission-control delivery state automation: phase-48

Локальная валидация:

```bash
python3 -m py_compile backend/app/admin_ui_kit.py backend/app/admin_fleet_page.py backend/app/admin_fleet_alerts_page.py backend/app/admin_fleet_incidents_page.py backend/app/main.py
python3 -m compileall -q backend/app
bash -n scripts/admin_panel_smoke_gate.sh
```

Выкатка на VPS:

```bash
rsync -av backend/app/admin_ui_kit.py alis@207.180.213.225:/opt/passengers-backend/app/
ssh alis@207.180.213.225 'cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml up -d --build api'
```

Smoke и delivery-automation marker checks:

```bash
./scripts/admin_panel_smoke_gate.sh --server-host 207.180.213.225 --server-user alis --admin-user admin --admin-pass "<BASIC_AUTH_PASS>"
for url in /admin/fleet /admin/fleet/alerts /admin/fleet/incidents /admin/audit; do
  echo "== ${url} =="
  curl -k -u "admin:<BASIC_AUTH_PASS>" -sS "https://207.180.213.225:8443${url}" | grep -E -c "buildMissionDeliveryAutomationPlan|renderMissionDeliveryAutomationSummary|apply-delivery-suggestion|bulk-ack-pending|bulk-retry-stale|bulk-escalate-stale|sideDeliveryAutomationSummary|sideDeliverySuggestedAction|collectMissionDeliveryBulkCandidates"
done
ssh alis@207.180.213.225 'sudo docker logs --tail 300 passengers-backend-api-1 2>&1 | grep -En "ResponseValidationError|Traceback|coroutine object|ERROR" || true'
```

## Модульный этап mission-control delivery policy profiles + UI cleanup: phase-49

Локальная валидация:

```bash
python3 -m py_compile backend/app/admin_ui_kit.py backend/app/admin_fleet_page.py backend/app/admin_fleet_alerts_page.py backend/app/admin_fleet_incidents_page.py backend/app/main.py
python3 -m compileall -q backend/app
bash -n scripts/admin_panel_smoke_gate.sh
```

Выкатка на VPS:

```bash
rsync -av backend/app/admin_ui_kit.py alis@207.180.213.225:/opt/passengers-backend/app/
ssh alis@207.180.213.225 'cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml up -d --build api'
```

Smoke и delivery-policy marker checks:

```bash
./scripts/admin_panel_smoke_gate.sh --server-host 207.180.213.225 --server-user alis --admin-user admin --admin-pass "<BASIC_AUTH_PASS>"
for url in /admin/fleet /admin/fleet/alerts /admin/fleet/incidents /admin/audit; do
  echo "== ${url} =="
  curl -k -u "admin:<BASIC_AUTH_PASS>" -sS "https://207.180.213.225:8443${url}" | grep -E -c "MISSION_DELIVERY_POLICY_STORAGE_KEY|MISSION_DELIVERY_POLICY_PROFILES|resolveMissionDeliveryPolicyProfile|renderMissionDeliveryPolicyControls|sideDeliveryPolicyProfile|sideDeliveryPolicySummary|apply-delivery-policy|sideSnapshotSection|sideSnapshotTools two"
done
ssh alis@207.180.213.225 'sudo docker logs --tail 320 passengers-backend-api-1 2>&1 | grep -En "ResponseValidationError|Traceback|coroutine object|ERROR" || true'
```

## Модульный этап mission-control UX refinement: phase-50

Локальная валидация:

```bash
python3 -m py_compile backend/app/admin_ui_kit.py backend/app/admin_fleet_page.py backend/app/admin_fleet_alerts_page.py backend/app/admin_fleet_incidents_page.py backend/app/main.py
python3 -m compileall -q backend/app
bash -n scripts/admin_panel_smoke_gate.sh
```

Выкатка на VPS:

```bash
rsync -av backend/app/admin_ui_kit.py alis@207.180.213.225:/opt/passengers-backend/app/
ssh alis@207.180.213.225 'cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml up -d --build api'
```

Smoke и delivery-ux marker checks:

```bash
./scripts/admin_panel_smoke_gate.sh --server-host 207.180.213.225 --server-user alis --admin-user admin --admin-pass "<BASIC_AUTH_PASS>"
for url in /admin/fleet /admin/fleet/alerts /admin/fleet/incidents /admin/audit; do
  echo "== ${url} =="
  curl -k -u "admin:<BASIC_AUTH_PASS>" -sS "https://207.180.213.225:8443${url}" | grep -E -c "sideDeliverySticky|sideDeliveryStickySuggested|runMissionHotkeyAction|confirmMissionDangerAction|apply-delivery-suggestion|ack-delivery|retry-delivery|escalate-delivery|bulk-ack-pending|bulk-retry-stale|bulk-escalate-stale"
done
ssh alis@207.180.213.225 'sudo docker logs --tail 320 passengers-backend-api-1 2>&1 | grep -En "ResponseValidationError|Traceback|coroutine object|ERROR" || true'
```

## Модульный этап mission-control navigation ergonomics: phase-51

Локальная валидация:

```bash
python3 -m py_compile backend/app/admin_ui_kit.py backend/app/admin_fleet_page.py backend/app/admin_fleet_alerts_page.py backend/app/admin_fleet_incidents_page.py backend/app/main.py
python3 -m compileall -q backend/app
bash -n scripts/admin_panel_smoke_gate.sh
```

Выкатка на VPS:

```bash
rsync -av backend/app/admin_ui_kit.py alis@207.180.213.225:/opt/passengers-backend/app/
ssh alis@207.180.213.225 'cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml up -d --build api'
```

Smoke и navigation marker checks:

```bash
./scripts/admin_panel_smoke_gate.sh --server-host 207.180.213.225 --server-user alis --admin-user admin --admin-pass "<BASIC_AUTH_PASS>"
rg -n "sideNavTools|sideNavSearch|sideNavCompactBtn|sideNavFilterStatus|applySidebarNavFilter|bindSidebarNavTools|applySidebarCompactMode" backend/app/admin_ui_kit.py
```

## Модульный этап mission-control navigation accessibility polish: phase-52

Локальная валидация:

```bash
python3 -m py_compile backend/app/admin_ui_kit.py backend/app/admin_fleet_page.py backend/app/admin_fleet_alerts_page.py backend/app/admin_fleet_incidents_page.py backend/app/main.py
python3 -m compileall -q backend/app
bash -n scripts/admin_panel_smoke_gate.sh
```

Выкатка на VPS:

```bash
rsync -av backend/app/admin_ui_kit.py alis@207.180.213.225:/opt/passengers-backend/app/
ssh alis@207.180.213.225 'cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml up -d --build api'
```

Smoke и navigation-accessibility marker checks:

```bash
./scripts/admin_panel_smoke_gate.sh --server-host 207.180.213.225 --server-user alis --admin-user admin --admin-pass "<BASIC_AUTH_PASS>"
ssh alis@207.180.213.225 'for url in /admin/fleet /admin/fleet/alerts /admin/fleet/incidents /admin/audit; do echo "== ${url} =="; curl -k -u "admin:<BASIC_AUTH_PASS>" -sS "https://127.0.0.1:8443${url}" | grep -E -c "sideNavHelpBtn|sideNavOnboarding|sideNavOnboardingDismiss|sideNavFilterStatus|sideNavSearch|sideNavCompactBtn|Shift\\+Alt\\+N|aria-live|sideNavOnboardingTitle"; done'
ssh alis@207.180.213.225 'sudo docker logs --tail 320 passengers-backend-api-1 2>&1 | grep -En "ResponseValidationError|Traceback|coroutine object|ERROR" || true'
```

## Модульный этап mission-control operator adoption telemetry: phase-53

Локальная валидация:

```bash
python3 -m py_compile backend/app/admin_ui_kit.py backend/app/admin_fleet_page.py backend/app/admin_fleet_alerts_page.py backend/app/admin_fleet_incidents_page.py backend/app/main.py
python3 -m compileall -q backend/app
bash -n scripts/admin_panel_smoke_gate.sh
```

Выкатка на VPS:

```bash
rsync -av backend/app/admin_ui_kit.py alis@207.180.213.225:/opt/passengers-backend/app/
ssh alis@207.180.213.225 'cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml up -d --build api'
```

Smoke и adoption-telemetry marker checks:

```bash
./scripts/admin_panel_smoke_gate.sh --server-host 207.180.213.225 --server-user alis --admin-user admin --admin-pass "<BASIC_AUTH_PASS>"
ssh alis@207.180.213.225 'for url in /admin/fleet /admin/fleet/alerts /admin/fleet/incidents /admin/audit; do echo "== ${url} =="; curl -k -u "admin:<BASIC_AUTH_PASS>" -sS "https://127.0.0.1:8443${url}" | grep -E -c "NAV_ADOPTION_STORAGE_KEY|NAV_ADOPTION_EVENT_LABELS|sideHandoffAdoption|buildSidebarNavAdoptionSummary|recordSidebarNavAdoptionEvent|renderMissionHandoffAdoptionSummary|setSidebarNavOnboardingVisible|Shift\\+Alt\\+N|nav_search_enter_open|nav_compact_enable"; done'
ssh alis@207.180.213.225 'sudo docker logs --tail 320 passengers-backend-api-1 2>&1 | grep -En "ResponseValidationError|Traceback|coroutine object|ERROR" || true'
```

## Модульный этап mission-control operator coaching loop: phase-54

Локальная валидация:

```bash
python3 -m py_compile backend/app/admin_ui_kit.py backend/app/admin_fleet_page.py backend/app/admin_fleet_alerts_page.py backend/app/admin_fleet_incidents_page.py backend/app/main.py
python3 -m compileall -q backend/app
bash -n scripts/admin_panel_smoke_gate.sh
```

Выкатка на VPS:

```bash
rsync -av backend/app/admin_ui_kit.py alis@207.180.213.225:/opt/passengers-backend/app/
ssh alis@207.180.213.225 'cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml up -d --build api'
```

Smoke и coaching-loop marker checks:

```bash
./scripts/admin_panel_smoke_gate.sh --server-host 207.180.213.225 --server-user alis --admin-user admin --admin-pass "<BASIC_AUTH_PASS>"
ssh alis@207.180.213.225 'for url in /admin/fleet /admin/fleet/alerts /admin/fleet/incidents /admin/audit; do echo "== ${url} =="; curl -k -u "admin:<BASIC_AUTH_PASS>" -sS "https://127.0.0.1:8443${url}" | grep -E -c "sideHandoffCoaching|runMissionHandoffAdoptionAction|data-handoff-adoption-action|buildSidebarNavAdoptionSnapshot|clearSidebarNavAdoption|adoptionEventCount|buildSidebarNavAdoptionCoaching"; done'
ssh alis@207.180.213.225 'sudo docker logs --tail 320 passengers-backend-api-1 2>&1 | grep -En "ResponseValidationError|Traceback|coroutine object|ERROR" || true'
```

## Модульный этап mission-control coaching cadence scoreboard: phase-55

Локальная валидация:

```bash
python3 -m py_compile backend/app/admin_ui_kit.py backend/app/admin_fleet_page.py backend/app/admin_fleet_alerts_page.py backend/app/admin_fleet_incidents_page.py backend/app/main.py
python3 -m compileall -q backend/app
bash -n scripts/admin_panel_smoke_gate.sh
```

Выкатка на VPS:

```bash
rsync -av backend/app/admin_ui_kit.py alis@207.180.213.225:/opt/passengers-backend/app/
ssh alis@207.180.213.225 'cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml up -d --build api'
```

Smoke и coaching-scoreboard marker checks:

```bash
./scripts/admin_panel_smoke_gate.sh --server-host 207.180.213.225 --server-user alis --admin-user admin --admin-pass "<BASIC_AUTH_PASS>"
ssh alis@207.180.213.225 'for url in /admin/fleet /admin/fleet/alerts /admin/fleet/incidents /admin/audit; do echo "== ${url} =="; curl -k -u "admin:<BASIC_AUTH_PASS>" -sS "https://127.0.0.1:8443${url}" | grep -E -c "sideHandoffScorecard|sideHandoffNextActions|buildSidebarNavAdoptionScorecard|buildSidebarNavNextActions|next_actions|Scorecard:"; done'
ssh alis@207.180.213.225 'sudo docker logs --tail 320 passengers-backend-api-1 2>&1 | grep -En "ResponseValidationError|Traceback|coroutine object|ERROR" || true'
```

## Модульный этап mission-control coaching trendline history: phase-56

Локальная валидация:

```bash
python3 -m py_compile backend/app/admin_ui_kit.py backend/app/admin_fleet_page.py backend/app/admin_fleet_alerts_page.py backend/app/admin_fleet_incidents_page.py backend/app/main.py
python3 -m compileall -q backend/app
bash -n scripts/admin_panel_smoke_gate.sh
```

Выкатка на VPS:

```bash
rsync -av backend/app/admin_ui_kit.py alis@207.180.213.225:/opt/passengers-backend/app/
ssh alis@207.180.213.225 'cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml up -d --build api'
```

Smoke и coaching-trend marker checks:

```bash
./scripts/admin_panel_smoke_gate.sh --server-host 207.180.213.225 --server-user alis --admin-user admin --admin-pass "<BASIC_AUTH_PASS>"
ssh alis@207.180.213.225 'for url in /admin/fleet /admin/fleet/alerts /admin/fleet/incidents /admin/audit; do echo "== ${url} =="; curl -k -u "admin:<BASIC_AUTH_PASS>" -sS "https://127.0.0.1:8443${url}" | grep -E -c "MISSION_ADOPTION_HISTORY_STORAGE_KEY|loadMissionAdoptionHistory|appendMissionAdoptionHistoryEntry|buildMissionAdoptionTrend|renderMissionHandoffAdoptionTrend|sideHandoffTrend|improving|regressing"; done'
ssh alis@207.180.213.225 'sudo docker logs --tail 320 passengers-backend-api-1 2>&1 | grep -En "ResponseValidationError|Traceback|coroutine object|ERROR" || true'
```

## Модульный этап mission-control coaching reset-pack: phase-57

Локальная валидация:

```bash
python3 -m py_compile backend/app/admin_ui_kit.py backend/app/admin_fleet_page.py backend/app/admin_fleet_alerts_page.py backend/app/admin_fleet_incidents_page.py backend/app/main.py
python3 -m compileall -q backend/app
bash -n scripts/admin_panel_smoke_gate.sh
```

Выкатка на VPS:

```bash
rsync -av backend/app/admin_ui_kit.py alis@207.180.213.225:/opt/passengers-backend/app/
ssh alis@207.180.213.225 'cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml up -d --build api'
```

Smoke и coaching-reset marker checks:

```bash
./scripts/admin_panel_smoke_gate.sh --server-host 207.180.213.225 --server-user alis --admin-user admin --admin-pass "<BASIC_AUTH_PASS>"
ssh alis@207.180.213.225 'for url in /admin/fleet /admin/fleet/alerts /admin/fleet/incidents /admin/audit; do echo "== ${url} =="; curl -k -u "admin:<BASIC_AUTH_PASS>" -sS "https://127.0.0.1:8443${url}" | grep -E -c "MISSION_ADOPTION_HISTORY_META_STORAGE_KEY|loadMissionAdoptionHistoryMeta|clearMissionAdoptionHistory|buildMissionAdoptionHistoryLifecycle|runMissionHandoffTrendAction|data-handoff-trend-action|sideHandoffTrendStatus|nav_trend_history_show|nav_trend_history_export|nav_trend_history_clear"; done'
ssh alis@207.180.213.225 'sudo docker logs --tail 320 passengers-backend-api-1 2>&1 | grep -En "ResponseValidationError|Traceback|coroutine object|ERROR" || true'
```

## Модульный этап mission-control coaching trend presets: phase-58

Локальная валидация:

```bash
python3 -m py_compile backend/app/admin_ui_kit.py backend/app/admin_fleet_page.py backend/app/admin_fleet_alerts_page.py backend/app/admin_fleet_incidents_page.py backend/app/main.py
python3 -m compileall -q backend/app
bash -n scripts/admin_panel_smoke_gate.sh
```

Выкатка на VPS:

```bash
rsync -av backend/app/admin_ui_kit.py alis@207.180.213.225:/opt/passengers-backend/app/
ssh alis@207.180.213.225 'cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml up -d --build api'
```

Smoke и trend-presets marker checks:

```bash
./scripts/admin_panel_smoke_gate.sh --server-host 207.180.213.225 --server-user alis --admin-user admin --admin-pass "<BASIC_AUTH_PASS>"
ssh alis@207.180.213.225 'for url in /admin/fleet /admin/fleet/alerts /admin/fleet/incidents /admin/audit; do echo "== ${url} =="; curl -k -u "admin:<BASIC_AUTH_PASS>" -sS "https://127.0.0.1:8443${url}" | grep -E -c "MISSION_ADOPTION_TREND_WINDOW_STORAGE_KEY|MISSION_ADOPTION_TREND_WINDOWS|loadMissionAdoptionTrendWindow|storeMissionAdoptionTrendWindow|buildMissionAdoptionCompareSummary|runMissionHandoffTrendWindowAction|data-handoff-trend-window|sideHandoffTrendCompare|nav_trend_window_set"; done'
ssh alis@207.180.213.225 'sudo docker logs --tail 320 passengers-backend-api-1 2>&1 | grep -En "ResponseValidationError|Traceback|coroutine object|ERROR" || true'
```

## Модульный этап mission-control coaching decision cues: phase-59

Локальная валидация:

```bash
python3 -m py_compile backend/app/admin_ui_kit.py backend/app/admin_fleet_page.py backend/app/admin_fleet_alerts_page.py backend/app/admin_fleet_incidents_page.py backend/app/main.py
python3 -m compileall -q backend/app
bash -n scripts/admin_panel_smoke_gate.sh
```

Выкатка на VPS:

```bash
rsync -av backend/app/admin_ui_kit.py alis@207.180.213.225:/opt/passengers-backend/app/
ssh alis@207.180.213.225 'cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml up -d --build api'
```

Smoke и decision-cues marker checks:

```bash
./scripts/admin_panel_smoke_gate.sh --server-host 207.180.213.225 --server-user alis --admin-user admin --admin-pass "<BASIC_AUTH_PASS>"
ssh alis@207.180.213.225 'for url in /admin/fleet /admin/fleet/alerts /admin/fleet/incidents /admin/audit; do echo "== ${url} =="; curl -k -u "admin:<BASIC_AUTH_PASS>" -sS "https://127.0.0.1:8443${url}" | grep -E -c "MISSION_ADOPTION_TREND_WINDOW_STORAGE_KEY|MISSION_ADOPTION_TREND_WINDOWS|loadMissionAdoptionTrendWindow|storeMissionAdoptionTrendWindow|buildMissionAdoptionCompareSummary|runMissionHandoffTrendWindowAction|data-handoff-trend-window|sideHandoffTrendCompare|buildMissionAdoptionTrendCoach|sideHandoffTrendCoach|trend_coach|nav_trend_window_set"; done'
ssh alis@207.180.213.225 'sudo docker logs --tail 320 passengers-backend-api-1 2>&1 | grep -En "ResponseValidationError|Traceback|coroutine object|ERROR" || true'
```

## Модульный этап mission-control coaching handoff composer: phase-60

Локальная валидация:

```bash
python3 -m py_compile backend/app/admin_ui_kit.py backend/app/admin_fleet_page.py backend/app/admin_fleet_alerts_page.py backend/app/admin_fleet_incidents_page.py backend/app/main.py
python3 -m compileall -q backend/app
bash -n scripts/admin_panel_smoke_gate.sh
```

Выкатка на VPS:

```bash
rsync -av backend/app/admin_ui_kit.py alis@207.180.213.225:/opt/passengers-backend/app/
ssh alis@207.180.213.225 'cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml up -d --build api'
```

Smoke и handoff-composer marker checks:

```bash
./scripts/admin_panel_smoke_gate.sh --server-host 207.180.213.225 --server-user alis --admin-user admin --admin-pass "<BASIC_AUTH_PASS>"
ssh alis@207.180.213.225 'for url in /admin/fleet /admin/fleet/alerts /admin/fleet/incidents /admin/audit; do echo "== ${url} =="; curl -k -u "admin:<BASIC_AUTH_PASS>" -sS "https://127.0.0.1:8443${url}" | grep -E -c "MISSION_HANDOFF_COMPOSER_STORAGE_KEY|buildMissionHandoffComposerTemplate|runMissionHandoffComposerAction|data-handoff-compose-action|sideHandoffComposer|compose_apply|compose_copy"; done'
ssh alis@207.180.213.225 'sudo docker logs --tail 320 passengers-backend-api-1 2>&1 | grep -En "ResponseValidationError|Traceback|coroutine object|ERROR" || true'
```

## Модульный этап mission-control handoff quality guard: phase-61

Локальная валидация:

```bash
python3 -m py_compile backend/app/admin_ui_kit.py backend/app/admin_fleet_page.py backend/app/admin_fleet_alerts_page.py backend/app/admin_fleet_incidents_page.py backend/app/main.py
python3 -m compileall -q backend/app
bash -n scripts/admin_panel_smoke_gate.sh
```

Выкатка на VPS:

```bash
rsync -av backend/app/admin_ui_kit.py alis@207.180.213.225:/opt/passengers-backend/app/
ssh alis@207.180.213.225 'cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml up -d --build api'
```

Smoke и quality-guard marker checks:

```bash
./scripts/admin_panel_smoke_gate.sh --server-host 207.180.213.225 --server-user alis --admin-user admin --admin-pass "<BASIC_AUTH_PASS>"
ssh alis@207.180.213.225 'for url in /admin/fleet /admin/fleet/alerts /admin/fleet/incidents /admin/audit; do echo "== ${url} =="; curl -k -u "admin:<BASIC_AUTH_PASS>" -sS "https://127.0.0.1:8443${url}" | grep -E -c "buildMissionHandoffQualityState|renderMissionHandoffQuality|runMissionHandoffQualityAction|data-handoff-quality-action|sideHandoffQuality|quality_blocked|quality_override|quality_check|Handoff NOT-READY"; done'
ssh alis@207.180.213.225 'sudo docker logs --tail 320 passengers-backend-api-1 2>&1 | grep -En "ResponseValidationError|Traceback|coroutine object|ERROR" || true'
```

Следующий этап (phase-62):

- добавить handoff quality profiles (`strict/balanced`) с переключателем policy;
- добавить явный policy-state в handoff quality summary;
- сохранить route/API contracts и текущие DOM hooks без backend schema изменений.

## Проверка архитектуры документации

Проверка навигации:

```bash
rg -n "Документация \\(архитектура и правила\\)|Шаблон документа модуля|Скиллы Codex" Docs/Проект/INDEX.md README.md
```

Проверка карты модулей docs:

```bash
find Docs/Проект -maxdepth 2 -type f | sort
```

Проверка отсутствия смешивания ролей в новых документах:

```bash
rg -n "## Назначение|## Контракт|## Операции|## Типовые проблемы" Docs/Проект/*.md
```

## Модульный этап mission-control handoff quality profiles: phase-62

Локальная валидация:

```bash
python3 -m py_compile backend/app/admin_ui_kit.py backend/app/admin_fleet_page.py backend/app/admin_fleet_alerts_page.py backend/app/admin_fleet_incidents_page.py backend/app/main.py
python3 -m compileall -q backend/app
bash -n scripts/admin_panel_smoke_gate.sh
```

Выкатка на VPS:

```bash
rsync -av backend/app/admin_ui_kit.py alis@207.180.213.225:/opt/passengers-backend/app/
ssh alis@207.180.213.225 'cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml up -d --build api'
```

Smoke и quality-profile marker checks:

```bash
./scripts/admin_panel_smoke_gate.sh --server-host 207.180.213.225 --server-user alis --admin-user admin --admin-pass "<BASIC_AUTH_PASS>"
ssh alis@207.180.213.225 'for url in /admin/fleet /admin/fleet/alerts /admin/fleet/incidents /admin/audit; do echo "== ${url} =="; curl -k -u "admin:<BASIC_AUTH_PASS>" -sS "https://127.0.0.1:8443${url}" | grep -E -c "MISSION_HANDOFF_QUALITY_PROFILE_STORAGE_KEY|MISSION_HANDOFF_QUALITY_PROFILES|normalizeMissionHandoffQualityProfileId|resolveMissionHandoffQualityProfile|runMissionHandoffQualityProfileAction|data-handoff-quality-profile|sideHandoffQualityPolicy|quality_profile_set|policy="; done'
ssh alis@207.180.213.225 'sudo docker logs --tail 320 passengers-backend-api-1 2>&1 | grep -En "ResponseValidationError|Traceback|coroutine object|ERROR" || true'
```

Следующий этап (phase-63):

- добавить one-click remediation в handoff quality (готовые действия по missing-пунктам);
- добавить explainability в quality summary (почему save заблокирован и что исправить);
- сохранить route/API contracts и текущие DOM hooks без backend schema изменений.

## Модульный этап mission-control handoff remediation assistant: phase-63

Локальная валидация:

```bash
python3 -m py_compile backend/app/admin_ui_kit.py backend/app/admin_fleet_page.py backend/app/admin_fleet_alerts_page.py backend/app/admin_fleet_incidents_page.py backend/app/main.py
python3 -m compileall -q backend/app
bash -n scripts/admin_panel_smoke_gate.sh
```

Выкатка на VPS:

```bash
rsync -av backend/app/admin_ui_kit.py alis@207.180.213.225:/opt/passengers-backend/app/
ssh alis@207.180.213.225 'cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml up -d --build api'
```

Smoke и remediation marker checks:

```bash
./scripts/admin_panel_smoke_gate.sh --server-host 207.180.213.225 --server-user alis --admin-user admin --admin-pass "<BASIC_AUTH_PASS>"
ssh alis@207.180.213.225 'for url in /admin/fleet /admin/fleet/alerts /admin/fleet/incidents /admin/audit; do echo "== ${url} =="; curl -k -u "admin:<BASIC_AUTH_PASS>" -sS "https://127.0.0.1:8443${url}" | grep -E -c "runMissionHandoffQualityRemediationAction|data-handoff-quality-remedy|sideHandoffQualityExplain|quality_remediate|appendMissionHandoffNextActionsToNote|appendMissionHandoffExpansionLine|fix="; done'
ssh alis@207.180.213.225 'sudo docker logs --tail 320 passengers-backend-api-1 2>&1 | grep -En "ResponseValidationError|Traceback|coroutine object|ERROR" || true'
```

Следующий этап (phase-64):

- добавить telemetry summary по remediation lifecycle (`applied/skipped/override-after-remediation/time-to-ready`);
- добавить compact KPI-блок в handoff для операторской смены;
- сохранить route/API contracts и текущие DOM hooks без backend schema изменений.

## Модульный этап mission-control handoff remediation telemetry: phase-64

Локальная валидация:

```bash
python3 -m py_compile backend/app/admin_ui_kit.py backend/app/admin_fleet_page.py backend/app/admin_fleet_alerts_page.py backend/app/admin_fleet_incidents_page.py backend/app/main.py
python3 -m compileall -q backend/app
bash -n scripts/admin_panel_smoke_gate.sh
```

Выкатка на VPS:

```bash
rsync -av backend/app/admin_ui_kit.py alis@207.180.213.225:/opt/passengers-backend/app/
ssh alis@207.180.213.225 'cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml up -d --build api'
```

Smoke и remediation-telemetry marker checks:

```bash
./scripts/admin_panel_smoke_gate.sh --server-host 207.180.213.225 --server-user alis --admin-user admin --admin-pass "<BASIC_AUTH_PASS>"
ssh alis@207.180.213.225 'for url in /admin/fleet /admin/fleet/alerts /admin/fleet/incidents /admin/audit; do echo "== ${url} =="; curl -k -u "admin:<BASIC_AUTH_PASS>" -sS "https://127.0.0.1:8443${url}" | grep -E -c "MISSION_HANDOFF_REMEDIATION_METRICS_STORAGE_KEY|MISSION_HANDOFF_REMEDIATION_TTR_LIMIT|normalizeMissionHandoffRemediationMetrics|updateMissionHandoffRemediationMetrics|renderMissionHandoffRemediationSummary|sideHandoffRemediationSummary|quality_remediate"; done'
ssh alis@207.180.213.225 'sudo docker logs --tail 320 passengers-backend-api-1 2>&1 | grep -En "ResponseValidationError|Traceback|coroutine object|ERROR" || true'
```

Следующий этап (phase-65):

- добавить remediation timeline drilldown по последним сменам (`last N cycles`);
- добавить export JSON remediation KPI с акцентом на override-cases;
- сохранить route/API contracts и текущие DOM hooks без backend schema изменений.

## Модульный этап mission-control handoff remediation timeline drilldown: phase-65

Локальная валидация:

```bash
python3 -m py_compile backend/app/admin_ui_kit.py backend/app/admin_fleet_page.py backend/app/admin_fleet_alerts_page.py backend/app/admin_fleet_incidents_page.py backend/app/main.py
python3 -m compileall -q backend/app
bash -n scripts/admin_panel_smoke_gate.sh
```

Выкатка на VPS:

```bash
rsync -av backend/app/admin_ui_kit.py alis@207.180.213.225:/opt/passengers-backend/app/
ssh alis@207.180.213.225 'cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml up -d --build api'
```

Smoke и remediation-timeline marker checks:

```bash
./scripts/admin_panel_smoke_gate.sh --server-host 207.180.213.225 --server-user alis --admin-user admin --admin-pass "<BASIC_AUTH_PASS>"
ssh alis@207.180.213.225 'for url in /admin/fleet /admin/fleet/alerts /admin/fleet/incidents /admin/audit; do echo "== ${url} =="; curl -k -u "admin:<BASIC_AUTH_PASS>" -sS "https://127.0.0.1:8443${url}" | grep -E -c "MISSION_HANDOFF_REMEDIATION_TIMELINE_STORAGE_KEY|MISSION_HANDOFF_REMEDIATION_TIMELINE_LIMIT|normalizeMissionHandoffRemediationTimelineItem|appendMissionHandoffRemediationTimelineEvent|buildMissionHandoffRemediationTimelineSummary|renderMissionHandoffRemediationTimelinePreview|runMissionHandoffRemediationHistoryAction|data-handoff-remediation-history-action|sideHandoffRemediationTimelineMeta|sideHandoffRemediationTimeline"; done'
ssh alis@207.180.213.225 'sudo docker logs --tail 320 passengers-backend-api-1 2>&1 | grep -En "ResponseValidationError|Traceback|coroutine object|ERROR" || true'
```

Следующий этап (phase-66):

- добавить governance targets для remediation KPI (`override/ttr` пороги) и compliance-state;
- добавить guided next-actions по отклонению от target;
- сохранить route/API contracts и текущие DOM hooks без backend schema изменений.

## Модульный этап mission-control handoff remediation governance pack: phase-66

Локальная валидация:

```bash
python3 -m py_compile backend/app/admin_ui_kit.py backend/app/admin_fleet_page.py backend/app/admin_fleet_alerts_page.py backend/app/admin_fleet_incidents_page.py backend/app/main.py
python3 -m compileall -q backend/app
bash -n scripts/admin_panel_smoke_gate.sh
```

Выкатка на VPS:

```bash
rsync -av backend/app/admin_ui_kit.py alis@207.180.213.225:/opt/passengers-backend/app/
ssh alis@207.180.213.225 'cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml up -d --build api'
```

Smoke и remediation-governance marker checks:

```bash
./scripts/admin_panel_smoke_gate.sh --server-host 207.180.213.225 --server-user alis --admin-user admin --admin-pass "<BASIC_AUTH_PASS>"
ssh alis@207.180.213.225 'for url in /admin/fleet /admin/fleet/alerts /admin/fleet/incidents /admin/audit; do echo "== ${url} =="; curl -k -u "admin:<BASIC_AUTH_PASS>" -sS "https://127.0.0.1:8443${url}" | grep -E -c "MISSION_HANDOFF_REMEDIATION_GOVERNANCE_STORAGE_KEY|MISSION_HANDOFF_REMEDIATION_GOVERNANCE_PROFILES|normalizeMissionHandoffRemediationGovernanceProfileId|buildMissionHandoffRemediationGovernanceState|renderMissionHandoffRemediationGovernance|runMissionHandoffRemediationGovernanceAction|data-handoff-remediation-governance|sideHandoffRemediationGovernance|sideHandoffRemediationActions"; done'
ssh alis@207.180.213.225 'sudo docker logs --tail 320 passengers-backend-api-1 2>&1 | grep -En "ResponseValidationError|Traceback|coroutine object|ERROR" || true'
```

Следующий этап (phase-67):

- добавить governance incidents при `WARN` (локальный incident feed в sidebar);
- добавить actions `ack/snooze` для governance incidents и trace в handoff timeline;
- сохранить route/API contracts и текущие DOM hooks без backend schema изменений.


## Модульный этап mission-control handoff remediation governance incidents: phase-67

Локальная валидация:

```bash
python3 -m py_compile backend/app/admin_ui_kit.py backend/app/admin_fleet_page.py backend/app/admin_fleet_alerts_page.py backend/app/admin_fleet_incidents_page.py backend/app/main.py
python3 -m compileall -q backend/app
bash -n scripts/admin_panel_smoke_gate.sh
```

Выкатка на VPS:

```bash
rsync -av backend/app/admin_ui_kit.py alis@207.180.213.225:/opt/passengers-backend/app/
ssh alis@207.180.213.225 'cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml up -d --build api'
```

Smoke и remediation-incidents marker checks:

```bash
./scripts/admin_panel_smoke_gate.sh --server-host 207.180.213.225 --server-user alis --admin-user admin --admin-pass "<BASIC_AUTH_PASS>"
ssh alis@207.180.213.225 'for url in /admin/fleet /admin/fleet/alerts /admin/fleet/incidents /admin/audit; do echo "== ${url} =="; curl -k -u "admin:<BASIC_AUTH_PASS>" -sS "https://127.0.0.1:8443${url}" | grep -E -c "MISSION_HANDOFF_REMEDIATION_INCIDENTS_STORAGE_KEY|MISSION_HANDOFF_REMEDIATION_INCIDENTS_LIMIT|MISSION_HANDOFF_REMEDIATION_INCIDENT_SNOOZE_MIN|normalizeMissionHandoffRemediationIncidentItem|listMissionHandoffRemediationIncidents|ensureMissionHandoffRemediationGovernanceIncident|runMissionHandoffRemediationIncidentAction|data-handoff-remediation-incident-action|sideHandoffRemediationIncidentSummary|sideHandoffRemediationIncidentFeed"; done'
ssh alis@207.180.213.225 'sudo docker logs --tail 320 passengers-backend-api-1 2>&1 | grep -En "ResponseValidationError|Traceback|coroutine object|ERROR" || true'
```

Следующий этап (phase-68):

- добавить operations panel для governance incidents: `show/export/clear` и SLA summary (`active/snoozed/acked`, oldest age);
- добавить timeline trace для journal действий incidents;
- сохранить route/API contracts и текущие DOM hooks без backend schema изменений.


## Модульный этап mission-control handoff remediation incidents operations panel: phase-68

Локальная валидация:

```bash
python3 -m py_compile backend/app/admin_ui_kit.py backend/app/admin_fleet_page.py backend/app/admin_fleet_alerts_page.py backend/app/admin_fleet_incidents_page.py backend/app/main.py
python3 -m compileall -q backend/app
bash -n scripts/admin_panel_smoke_gate.sh
```

Выкатка на VPS:

```bash
rsync -av backend/app/admin_ui_kit.py alis@207.180.213.225:/opt/passengers-backend/app/
ssh alis@207.180.213.225 'cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml up -d --build api'
```

Smoke и phase-68 marker checks:

```bash
./scripts/admin_panel_smoke_gate.sh --server-host 207.180.213.225 --server-user alis --admin-user admin --admin-pass "<BASIC_AUTH_PASS>"
ssh alis@207.180.213.225 'for url in /admin/fleet /admin/fleet/alerts /admin/fleet/incidents /admin/audit; do echo "== ${url} =="; curl -k -u "admin:<BASIC_AUTH_PASS>" -sS "https://127.0.0.1:8443${url}" | grep -E -c "buildMissionHandoffRemediationIncidentSlaSummary|renderMissionHandoffRemediationIncidentSla|runMissionHandoffRemediationIncidentHistoryAction|data-handoff-remediation-incident-history-action|sideHandoffRemediationIncidentSla|remediation_incident_journal_show|remediation_incident_journal_export|remediation_incident_journal_clear"; done'
ssh alis@207.180.213.225 'sudo docker logs --tail 320 passengers-backend-api-1 2>&1 | grep -En "ResponseValidationError|Traceback|coroutine object|ERROR" || true'
```

Следующий этап (phase-69):

- добавить incidents digest board: top fingerprints (`count/age/last-state`) и compact risk ordering;
- добавить guided triage hints по top-repeat patterns;
- сохранить route/API contracts и текущие DOM hooks без backend schema изменений.


## Модульный этап mission-control handoff remediation incidents digest board: phase-69

Локальная валидация:

```bash
python3 -m py_compile backend/app/admin_ui_kit.py backend/app/admin_fleet_page.py backend/app/admin_fleet_alerts_page.py backend/app/admin_fleet_incidents_page.py backend/app/main.py
python3 -m compileall -q backend/app
bash -n scripts/admin_panel_smoke_gate.sh
```

Выкатка на VPS:

```bash
rsync -av backend/app/admin_ui_kit.py alis@207.180.213.225:/opt/passengers-backend/app/
ssh alis@207.180.213.225 'cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml up -d --build api'
```

Smoke и phase-69 marker checks:

```bash
./scripts/admin_panel_smoke_gate.sh --server-host 207.180.213.225 --server-user alis --admin-user admin --admin-pass "<BASIC_AUTH_PASS>"
ssh alis@207.180.213.225 'for url in /admin/fleet /admin/fleet/alerts /admin/fleet/incidents /admin/audit; do echo "== ${url} =="; curl -k -u "admin:<BASIC_AUTH_PASS>" -sS "https://127.0.0.1:8443${url}" | grep -E -c "buildMissionHandoffRemediationIncidentDigest|buildMissionHandoffRemediationIncidentTriageHints|renderMissionHandoffRemediationIncidentDigest|sideHandoffRemediationIncidentDigest|sideHandoffRemediationIncidentTriage|Top fingerprint|Digest triage"; done'
ssh alis@207.180.213.225 'sudo docker logs --tail 320 passengers-backend-api-1 2>&1 | grep -En "ResponseValidationError|Traceback|coroutine object|ERROR" || true'
```

Следующий этап (phase-70):

- добавить digest action planner с suggested actions по top fingerprints (`ack/snooze/escalate/profile-check`);
- добавить генерацию handoff-ready action plan для смены из digest;
- сохранить route/API contracts и текущие DOM hooks без backend schema изменений.


## Модульный этап mission-control handoff remediation digest action planner: phase-70

Локальная валидация:

```bash
python3 -m py_compile backend/app/admin_ui_kit.py backend/app/admin_fleet_page.py backend/app/admin_fleet_alerts_page.py backend/app/admin_fleet_incidents_page.py backend/app/main.py
python3 -m compileall -q backend/app
bash -n scripts/admin_panel_smoke_gate.sh
```

Выкатка на VPS:

```bash
rsync -av backend/app/admin_ui_kit.py alis@207.180.213.225:/opt/passengers-backend/app/
ssh alis@207.180.213.225 'cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml up -d --build api'
```

Smoke и phase-70 marker checks:

```bash
./scripts/admin_panel_smoke_gate.sh --server-host 207.180.213.225 --server-user alis --admin-user admin --admin-pass "<BASIC_AUTH_PASS>"
ssh alis@207.180.213.225 'for url in /admin/fleet /admin/fleet/alerts /admin/fleet/incidents /admin/audit; do echo "== ${url} =="; curl -k -u "admin:<BASIC_AUTH_PASS>" -sS "https://127.0.0.1:8443${url}" | grep -E -c "missionHandoffRemediationPlannerSuggestAction|buildMissionHandoffRemediationActionPlan|renderMissionHandoffRemediationActionPlan|runMissionHandoffRemediationPlanAction|data-handoff-remediation-plan-action|sideHandoffRemediationPlanSummary|sideHandoffRemediationPlan|remediation_digest_plan_show|remediation_digest_plan_export|remediation_digest_plan_copy"; done'
ssh alis@207.180.213.225 'sudo docker logs --tail 320 passengers-backend-api-1 2>&1 | grep -En "ResponseValidationError|Traceback|coroutine object|ERROR" || true'
```

Следующий этап (phase-71):

- добавить planner decision ledger (show/export/clear) с записью выбранного action по fingerprint;
- добавить decision coverage summary по текущей смене;
- сохранить route/API contracts и текущие DOM hooks без backend schema изменений.


## Модульный этап mission-control remediation planner decision ledger: phase-71

Локальная валидация:

```bash
python3 -m py_compile backend/app/admin_ui_kit.py backend/app/admin_fleet_page.py backend/app/admin_fleet_alerts_page.py backend/app/admin_fleet_incidents_page.py backend/app/main.py
python3 -m compileall -q backend/app
bash -n scripts/admin_panel_smoke_gate.sh
```

Выкатка на VPS:

```bash
rsync -av backend/app/admin_ui_kit.py alis@207.180.213.225:/opt/passengers-backend/app/
ssh alis@207.180.213.225 'cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml up -d --build api'
```

Smoke и phase-71 marker checks:

```bash
./scripts/admin_panel_smoke_gate.sh --server-host 207.180.213.225 --server-user alis --admin-user admin --admin-pass "<BASIC_AUTH_PASS>"
ssh alis@207.180.213.225 'for url in /admin/fleet /admin/fleet/alerts /admin/fleet/incidents /admin/audit; do echo "== ${url} =="; curl -k -u "admin:<BASIC_AUTH_PASS>" -sS "https://127.0.0.1:8443${url}" | grep -E -c "MISSION_HANDOFF_REMEDIATION_DECISION_LEDGER_STORAGE_KEY|MISSION_HANDOFF_REMEDIATION_DECISION_LEDGER_LIMIT|normalizeMissionHandoffRemediationDecisionItem|listMissionHandoffRemediationDecisionLedger|buildMissionHandoffRemediationDecisionCoverage|renderMissionHandoffRemediationDecisionCoverage|runMissionHandoffRemediationPlanAction|data-handoff-remediation-plan-action|sideHandoffRemediationDecisionCoverage|sideHandoffRemediationDecisionLedger|remediation_decision_ledger_show|remediation_decision_ledger_export|remediation_decision_ledger_clear|remediation_digest_plan_log_primary"; done'
ssh alis@207.180.213.225 'sudo docker logs --tail 320 passengers-backend-api-1 2>&1 | grep -En "ResponseValidationError|Traceback|coroutine object|ERROR" || true'
```

Следующий этап (phase-72):

- добавить decision backlog cockpit по coverage-gap (`missing decisions`) с show/export/copy;
- добавить handoff-ready summary по незакрытым решениям planner;
- сохранить route/API contracts и текущие DOM hooks без backend schema изменений.


## Модульный этап mission-control remediation decision backlog cockpit: phase-72

Локальная валидация:

```bash
python3 -m py_compile backend/app/admin_ui_kit.py backend/app/admin_fleet_page.py backend/app/admin_fleet_alerts_page.py backend/app/admin_fleet_incidents_page.py backend/app/main.py
python3 -m compileall -q backend/app
bash -n scripts/admin_panel_smoke_gate.sh
```

Выкатка на VPS:

```bash
rsync -av backend/app/admin_ui_kit.py alis@207.180.213.225:/opt/passengers-backend/app/
ssh alis@207.180.213.225 'cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml up -d --build api'
```

Smoke и phase-72 marker checks:

```bash
./scripts/admin_panel_smoke_gate.sh --server-host 207.180.213.225 --server-user alis --admin-user admin --admin-pass "<BASIC_AUTH_PASS>"
ssh alis@207.180.213.225 'for url in /admin/fleet /admin/fleet/alerts /admin/fleet/incidents /admin/audit; do echo "== ${url} =="; curl -k -u "admin:<BASIC_AUTH_PASS>" -sS "https://127.0.0.1:8443${url}" | grep -E -c "missionHandoffRemediationDecisionMatchKey|buildMissionHandoffRemediationDecisionBacklog|renderMissionHandoffRemediationDecisionBacklog|runMissionHandoffRemediationPlanAction|data-handoff-remediation-plan-action|sideHandoffRemediationDecisionBacklogSummary|sideHandoffRemediationDecisionBacklog|backlog-show|backlog-export|backlog-copy|remediation_decision_backlog_show|remediation_decision_backlog_export|remediation_decision_backlog_copy"; done'
ssh alis@207.180.213.225 'sudo docker logs --tail 320 passengers-backend-api-1 2>&1 | grep -En "ResponseValidationError|Traceback|coroutine object|ERROR" || true'
```

Следующий этап (phase-73):

- добавить backlog closeout assistant с быстрыми действиями `ack/snooze/profile-check` прямо из missing-решений;
- добавить пакетный closeout по top-missing решениям для handoff;
- сохранить route/API contracts и текущие DOM hooks без backend schema изменений.


## Модульный этап mission-control remediation decision backlog closeout assistant: phase-73

Локальная валидация:

```bash
python3 -m py_compile backend/app/admin_ui_kit.py backend/app/admin_fleet_page.py backend/app/admin_fleet_alerts_page.py backend/app/admin_fleet_incidents_page.py backend/app/main.py
python3 -m compileall -q backend/app
bash -n scripts/admin_panel_smoke_gate.sh
```

Выкатка на VPS:

```bash
rsync -av backend/app/admin_ui_kit.py alis@207.180.213.225:/opt/passengers-backend/app/
ssh alis@207.180.213.225 'cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml up -d --build api'
```

Smoke и phase-73 marker checks:

```bash
./scripts/admin_panel_smoke_gate.sh --server-host 207.180.213.225 --server-user alis --admin-user admin --admin-pass "<BASIC_AUTH_PASS>"
ssh alis@207.180.213.225 'for url in /admin/fleet /admin/fleet/alerts /admin/fleet/incidents /admin/audit; do echo "== ${url} =="; curl -k -u "admin:<BASIC_AUTH_PASS>" -sS "https://127.0.0.1:8443${url}" | grep -E -c "MISSION_HANDOFF_REMEDIATION_DECISION_BACKLOG_CLOSEOUT_LIMIT|normalizeMissionHandoffRemediationBacklogDecision|applyMissionHandoffRemediationBacklogCloseout|runMissionHandoffRemediationPlanAction|data-handoff-remediation-plan-action|backlog-item-ack-|backlog-item-snooze-|backlog-item-profile-check-|backlog-batch-ack|backlog-batch-snooze|backlog-batch-profile-check|remediation_decision_backlog_closeout_item|remediation_decision_backlog_closeout_batch|sideHandoffRemediationDecisionBacklog"; done'
ssh alis@207.180.213.225 'sudo docker logs --tail 320 passengers-backend-api-1 2>&1 | grep -En "ResponseValidationError|Traceback|coroutine object|ERROR" || true'
```

Следующий этап (phase-74):

- добавить closeout governance board с KPI по `single/batch` и decision-type (`ack/snooze/profile-check`);
- добавить shift-ready JSON/export отчёт по closeout-циклу и remaining gap;
- сохранить route/API contracts и текущие DOM hooks без backend schema изменений.


## Модульный этап mission-control remediation decision closeout governance board: phase-74

Локальная валидация:

```bash
python3 -m py_compile backend/app/admin_ui_kit.py backend/app/admin_fleet_page.py backend/app/admin_fleet_alerts_page.py backend/app/admin_fleet_incidents_page.py backend/app/main.py
python3 -m compileall -q backend/app
bash -n scripts/admin_panel_smoke_gate.sh
```

Выкатка на VPS:

```bash
rsync -av backend/app/admin_ui_kit.py alis@207.180.213.225:/opt/passengers-backend/app/
ssh alis@207.180.213.225 'cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml up -d --build api'
```

Smoke и phase-74 marker checks:

```bash
./scripts/admin_panel_smoke_gate.sh --admin-pass "<BASIC_AUTH_PASS>"
ssh alis@207.180.213.225 'for url in /admin/fleet /admin/fleet/alerts /admin/fleet/incidents /admin/audit; do echo "==${url}=="; curl -k -u "admin:<BASIC_AUTH_PASS>" -sS "https://127.0.0.1:8443${url}" | grep -E -c "closeout-governance-show|closeout-governance-export|sideHandoffRemediationCloseoutSummary|sideHandoffRemediationCloseoutBoard|buildMissionHandoffRemediationCloseoutGovernance|renderMissionHandoffRemediationCloseoutGovernance|remediation_decision_closeout_governance_show|remediation_decision_closeout_governance_export"; done'
ssh alis@207.180.213.225 'sudo docker logs --tail 320 passengers-backend-api-1 2>&1 | grep -En "ResponseValidationError|Traceback|coroutine object|ERROR" || true'
```

Следующий этап (phase-75):

- добавить escalation routing для closeout governance по `status/remaining_gap`;
- добавить actions `show/export/copy` для escalation payload и handoff summary;
- добавить timeline events по escalation routing;
- сохранить route/API contracts и текущие DOM hooks без backend schema изменений.


## Модульный этап mission-control remediation closeout escalation routing: phase-75

Локальная валидация:

```bash
python3 -m py_compile backend/app/admin_ui_kit.py backend/app/admin_fleet_page.py backend/app/admin_fleet_alerts_page.py backend/app/admin_fleet_incidents_page.py backend/app/main.py
python3 -m compileall -q backend/app
bash -n scripts/admin_panel_smoke_gate.sh
```

Выкатка на VPS:

```bash
rsync -av backend/app/admin_ui_kit.py alis@207.180.213.225:/opt/passengers-backend/app/
ssh alis@207.180.213.225 'cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml up -d --build api'
```

Smoke и phase-75 marker checks:

```bash
./scripts/admin_panel_smoke_gate.sh --admin-pass "<BASIC_AUTH_PASS>"
ssh alis@207.180.213.225 'for url in /admin/fleet /admin/fleet/alerts /admin/fleet/incidents /admin/audit; do echo "==${url}=="; curl -k -u "admin:<BASIC_AUTH_PASS>" -sS "https://127.0.0.1:8443${url}" | grep -E -c "MISSION_HANDOFF_REMEDIATION_CLOSEOUT_ESCALATION_POLICIES|resolveMissionHandoffRemediationCloseoutEscalationPolicy|buildMissionHandoffRemediationCloseoutEscalation|renderMissionHandoffRemediationCloseoutEscalation|closeout-escalation-show|closeout-escalation-export|closeout-escalation-copy|sideHandoffRemediationEscalationSummary|sideHandoffRemediationEscalationBoard|remediation_decision_closeout_escalation_show|remediation_decision_closeout_escalation_export|remediation_decision_closeout_escalation_copy"; done'
ssh alis@207.180.213.225 'sudo docker logs --tail 320 passengers-backend-api-1 2>&1 | grep -En "ResponseValidationError|Traceback|coroutine object|ERROR" || true'
```

Следующий этап (phase-76):

- добавить execution logbook для escalation route (`ack/snooze/resolve`) в mission-control;
- добавить escalation SLA-age и stale-подсветку для контроля исполнения;
- добавить show/export/copy execution payload для handoff;
- сохранить route/API contracts и текущие DOM hooks без backend schema изменений.


## Модульный этап mission-control closeout escalation execution logbook: phase-76

Локальная валидация:

```bash
python3 -m py_compile backend/app/admin_ui_kit.py backend/app/admin_fleet_page.py backend/app/admin_fleet_alerts_page.py backend/app/admin_fleet_incidents_page.py backend/app/main.py
python3 -m compileall -q backend/app
bash -n scripts/admin_panel_smoke_gate.sh
```

Выкатка на VPS:

```bash
rsync -av backend/app/admin_ui_kit.py alis@207.180.213.225:/opt/passengers-backend/app/
ssh alis@207.180.213.225 'cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml up -d --build api'
```

Smoke и phase-76 marker checks:

```bash
./scripts/admin_panel_smoke_gate.sh --admin-pass "<BASIC_AUTH_PASS>"
ssh alis@207.180.213.225 'for url in /admin/fleet /admin/fleet/alerts /admin/fleet/incidents /admin/audit; do echo "==${url}=="; curl -k -u "admin:<BASIC_AUTH_PASS>" -sS "https://127.0.0.1:8443${url}" | grep -E -c "MISSION_HANDOFF_REMEDIATION_CLOSEOUT_ESCALATION_EXECUTION_STORAGE_KEY|MISSION_HANDOFF_REMEDIATION_CLOSEOUT_ESCALATION_EXECUTION_LIMIT|MISSION_HANDOFF_REMEDIATION_CLOSEOUT_ESCALATION_SLA_STALE_SEC|MISSION_HANDOFF_REMEDIATION_CLOSEOUT_ESCALATION_SNOOZE_MIN|normalizeMissionHandoffRemediationCloseoutEscalationExecutionItem|listMissionHandoffRemediationCloseoutEscalationExecution|storeMissionHandoffRemediationCloseoutEscalationExecution|appendMissionHandoffRemediationCloseoutEscalationExecutionEntry|missionHandoffRemediationCloseoutEscalationExecutionActive|buildMissionHandoffRemediationCloseoutEscalationExecutionState|renderMissionHandoffRemediationCloseoutEscalationExecution|closeout-escalation-ack|closeout-escalation-snooze|closeout-escalation-resolve|closeout-escalation-log-show|closeout-escalation-log-export|closeout-escalation-log-copy|sideHandoffRemediationEscalationExecutionSummary|sideHandoffRemediationEscalationExecutionBoard|remediation_decision_closeout_escalation_ack|remediation_decision_closeout_escalation_snooze|remediation_decision_closeout_escalation_resolve|remediation_decision_closeout_escalation_execution_show|remediation_decision_closeout_escalation_execution_export|remediation_decision_closeout_escalation_execution_copy"; done'
ssh alis@207.180.213.225 'sudo docker logs --tail 320 passengers-backend-api-1 2>&1 | grep -En "ResponseValidationError|Traceback|coroutine object|ERROR" || true'
```

Следующий этап (phase-77):

- добавить anomaly guard для execution logbook (`repeat snooze`, `long stale`, `unresolved route`);
- добавить recommended action controls и anomaly JSON/export в mission-control;
- добавить timeline traces по anomaly lifecycle;
- сохранить route/API contracts и текущие DOM hooks без backend schema изменений.
