# Проблемы и решения

## 1) Время не синхронизировано

Симптом:

- `timedatectl status` → `System clock synchronized: no`
- события приходят с неверным временем

Решение:

- идти по `Docs/настройка ПО/План/3. Синхронизация времени (критично для данных).md`

## 2) Не пингуется Door узел

Проверить:

- линк/кабель/свитч
- `ip -br addr` на Door (есть ли `192.168.10.11/12`)
- `nmcli con show --active` (активен ли `opizero3-static`)

## 3) SSH недоступен

Основной путь: UART/SD правка — `Docs/настройка ПО/Обязательно.md`.

## 4) Не создаётся venv

Причина: не установлен пакет `python3-venv` или нет прав.

Док: `Docs/настройка ПО/0 - базовая настройка ОС/П1 - Почему не создаётся venv.md`.

## 5) journald ругается на `system.journal corrupted`

См. разбор и варианты фикса:

- `Docs/настройка ПО/0 - базовая настройка ОС/П2 - В dmesg есть сообщения о повреждении system.journal и ротации — признак      некорректного выключения-ребута.md`

## 6) Central видит WG (ping `10.66.0.1`), но backend health недоступен

Симптом:

- `ping 10.66.0.1` c `central-gw` проходит
- `curl http://10.66.0.1/health` не проходит
- `passengers-central-uplink` зависает в preflight/рестартах

Причина:

- backend на сервере запущен без `compose.server.yaml`, и контейнер не слушает `10.66.0.1:80`

Решение:

```bash
ssh alis@207.180.213.225 'cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml up -d --build'
ssh orangepi@192.168.10.1 'curl -sS http://10.66.0.1/health'
```

## 7) В Fleet есть `door_events_stale`, но узел доступен

Симптом:

- в `/admin/fleet` дверь reachable=`true`
- есть `warn` алерт `door_events_stale`

Пояснение:

- это не “узел умер”, а отсутствие новых событий дольше порога (сейчас `>1800s`)
- часто бывает в простой/тесте без трафика людей

Проверка:

```bash
ssh orangepi@192.168.10.11 'systemctl is-active passengers-edge-sender'
ssh orangepi@192.168.10.12 'systemctl is-active passengers-edge-sender'
ssh orangepi@192.168.10.1 'sudo python3 /opt/passengers-mvp/enqueue_event.py --door-id 2 --in 1 --out 0'
```

## 8) `fleet_apply_wg_peer.py` не применяет peer из-за placeholder ключа

Симптом:

- `Peer block ... contains <CENTRAL_PUBLIC_KEY>`

Причина:

- в `fleet/out/<SYSTEM_ID>/wireguard/server-peer.conf` не подставлен реальный публичный ключ Central.

Решение (предпочтительно, автоматически):

```bash
python3 scripts/fleet_apply_wg_peer.py --system-id <SYSTEM_ID> --fetch-central-pubkey
```

Если ключ на Central ещё не создан:

```bash
python3 scripts/fleet_apply_wg_peer.py --system-id <SYSTEM_ID> --fetch-central-pubkey --ensure-central-key
```

Ручной fallback (если нужно):

```bash
ssh orangepi@192.168.10.1 'sudo cat /etc/wireguard/central.pub'
python3 scripts/fleet_apply_wg_peer.py --system-id <SYSTEM_ID> --central-public-key "<KEY>"
```

## 9) После отключения питания узел в `degraded`, но сервисы проекта `active`

Симптом:

- `systemctl is-system-running` показывает `degraded`
- `passengers-*` и `chrony` при этом `active`

Типовые причины на OPi:

- `smartmontools.service` (на SD-картах часто не нужен)
- `dnsmasq.service` (в текущем проекте не нужен; на `central-gw` может конфликтовать с `systemd-resolved` на порту 53)

Решение:

```bash
for host in 192.168.10.1 192.168.10.11 192.168.10.12; do
  ssh orangepi@$host 'sudo systemctl disable --now smartmontools.service dnsmasq.service; sudo systemctl reset-failed'
done
```

## 10) `passengers-central-flush.timer` не `active`, но система работает

Симптом:

- на `central-gw` `systemctl is-active passengers-central-flush.timer` -> `inactive`
- `passengers-collector` и `passengers-central-uplink` активны

Причина:

- включён режим `STOP_MODE=manual` (авто flush отключён штатно)

Проверка:

```bash
ssh orangepi@192.168.10.1 'sudo sed -n "s/^STOP_MODE=//p" /etc/passengers/passengers.env | head -n1'
```

Решение:

- если нужен ручной режим — ничего не делать;
- если нужен авто flush:

```bash
ssh orangepi@192.168.10.1 "sudo sed -i 's/^STOP_MODE=.*/STOP_MODE=timer/' /etc/passengers/passengers.env && sudo systemctl enable --now passengers-central-flush.timer && sudo systemctl restart passengers-central-uplink passengers-central-heartbeat.timer"
```

## 11) Dry-run масштаба падает на duplicate `vehicle_id`/`wg_ip`

Симптом:

- `python3 scripts/fleet_scale_dry_run.py --target-systems 200` выдаёт `FAIL`
- в отчёте есть ошибки duplicate/invalid из `fleet/registry.csv`

Решение:

```bash
python3 scripts/fleet_registry.py validate
python3 scripts/fleet_registry.py next-wg-ip
```

Далее исправить конфликтные строки в `fleet/registry.csv` и повторить dry-run.

## 12) После ротации ключей Central получает `401 unauthorized`

Симптом:

- `passengers-central-uplink` активен, но в логах повторяются ошибки `http_401`
- backend принимает только новые ключи из `PASSENGERS_API_KEYS`

Решение:

```bash
python3 scripts/fleet_api_keys.py sync-server --server-host 207.180.213.225 --server-user alis
python3 scripts/fleet_apply_central_env.py --system-id <SYSTEM_ID>
```

Проверка:

```bash
ssh orangepi@192.168.10.1 'sudo journalctl -u passengers-central-uplink -n 50 --no-pager'
```

## 13) Временная ошибка SSH к серверу: `No route to host`

Симптом:

- при частых SSH-подключениях подряд к `207.180.213.225:22` появляется `No route to host`
- ping при этом может отвечать

Причина:

- срабатывает ограничение `ufw` (`LIMIT IN`) по SSH и/или fail2ban после бурста подключений

Решение:

- подождать 20–60 секунд и повторить команду;
- избегать бурста параллельных SSH, использовать один оркестраторный скрипт;
- проверить jails на сервере:

```bash
ssh alis@207.180.213.225 'sudo fail2ban-client status'
```

## 14) После миграции `CENTRAL_ID` в fleet видны и `sys-0001`, и `central-gw`

Симптом:

- в `/api/admin/fleet/centrals` и в админке одновременно видны новый `central_id` и старый `central-gw`

Причина:

- это исторические heartbeat записи до миграции идентификатора; новые heartbeat уже идут с новым `CENTRAL_ID`.

Проверка:

```bash
ssh orangepi@192.168.10.1 "sudo sed -n 's/^CENTRAL_ID=//p' /etc/passengers/passengers.env | head -n1"
ssh orangepi@192.168.10.1 'sudo python3 /opt/passengers-mvp/central_heartbeat.py --timeout-sec 6'
ssh alis@207.180.213.225 'token=$(grep -m1 "^ADMIN_API_KEYS=" /opt/passengers-backend/.env | cut -d= -f2 | tr -d "\r" | cut -d, -f1); curl -sS -H "Authorization: Bearer ${token}" "http://10.66.0.1/api/admin/fleet/centrals" | jq -r ".centrals[]?.central_id"'
```

Что делать:

- использовать в работе и `overrides` только новый `central_id=<SYSTEM_ID>`;
- старый `central-gw` можно игнорировать до естественного устаревания записи в мониторинге.

## 15) Белый экран в админке `https://<server>:8443/admin`

Симптом:

- после ввода логина/пароля страница визуально пустая;
- API живой, но браузер показывает старый закэшированный HTML/JS.

Проверка:

```bash
curl -k -u admin:'<password>' -I https://207.180.213.225:8443/admin
curl -k -u admin:'<password>' "https://207.180.213.225:8443/api/admin/fleet/monitor?window=24h"
```

Ожидаемо в заголовках:

- `Cache-Control: no-store, no-cache, must-revalidate, proxy-revalidate, max-age=0`

Решение:

- открыть админку в новой приватной вкладке (или сделать hard refresh `Ctrl+Shift+R`);
- если не помогло, очистить site data для `207.180.213.225:8443`;
- проверить, что nginx конфиг содержит анти-кэш заголовки в `location ^~ /admin` и `location ^~ /api/admin/`.

Применение конфига на сервер:

```bash
scp server/nginx/passengers-admin.conf alis@207.180.213.225:/tmp/passengers-admin.conf
ssh alis@207.180.213.225 'sudo install -m 644 -o root -g root /tmp/passengers-admin.conf /etc/nginx/conf.d/passengers-admin.conf && sudo nginx -t && sudo systemctl reload nginx && sudo rm -f /tmp/passengers-admin.conf'
```

## 16) После ребута `systemctl is-system-running` показывает `degraded` из-за `passengers-service-watchdog.service`

Симптом:

- watchdog таймер активен, но система в `degraded`;
- в `systemctl --failed` виден `passengers-service-watchdog.service`.

Причина:

- `service_watchdog.py` возвращает код `2` при временном `degraded` в момент старта;
- без `SuccessExitStatus=2` systemd помечает oneshot сервис как `failed`.

Решение:

- в unit-файле `passengers-service-watchdog.service` указать `SuccessExitStatus=2`;
- перезагрузить systemd + сбросить failed state.

Команды:

```bash
for host in 192.168.10.1 192.168.10.11 192.168.10.12; do
  ssh orangepi@$host "sudo sed -i '/^ExecStart=.*service_watchdog.py/a SuccessExitStatus=2' /etc/systemd/system/passengers-service-watchdog.service; sudo systemctl daemon-reload; sudo systemctl reset-failed passengers-service-watchdog.service; sudo systemctl start passengers-service-watchdog.service"
done
```

## 17) Ложный `FAIL` при параллельном запуске resilience тестов

Симптом:

- `test_edge_central_resilience.sh` и `test_central_server_resilience.sh` запущены одновременно;
- один или оба теста завершаются `FAIL` без стабильной воспроизводимости.

Причина:

- тесты не независимы: оба вмешиваются в общий контур (`collector`, `wg`, flush/pending), поэтому параллельный запуск искажает сценарий.

Решение:

- запускать resilience тесты строго последовательно;
- рекомендованный порядок: `edge→central`, потом `central→server`, затем `mvp_e2e_smoke.sh`.

## 18) Белый экран в `https://<server>:8443/admin/fleet` из-за JS-ошибки

Симптом:

- `/admin/fleet` открывается, но контент пустой (белый экран);
- в консоли браузера ошибка вида `Cannot set properties of null` для `prio...`/`focus...`.

Причина:

- блок `Оперативний пріоритет` (элементы `prioBadNodes`, `focusBad`, `fleetFocusState`) отсутствует в `/admin/fleet` или случайно размещён в `/admin/wg`;
- JS в `/admin/fleet` ожидает эти `id` и падает на обновлении.

Проверка:

```bash
cd /opt/passengers-backend
rg -n "Оперативний пріоритет|prioBadNodes|focusBad|@app.get\\(\"/admin/wg\"|@app.get\\(\"/admin/fleet\"" backend/app/main.py
python3 -m py_compile backend/app/main.py
```

Решение:

- держать блок `Оперативний пріоритет` только в route `/admin/fleet`;
- в `/admin/wg` оставить только таблицу peers и конфиг WG;
- после правки пересобрать/перезапустить backend и сделать hard refresh страницы.

## 19) После деплоя новый route/API не появляется (например `/admin/fleet/alerts`)

Симптом:

- в локальном коде route есть, но на сервере `404 Not Found`;
- существующие старые route продолжают работать.

Причина:

- на сервер в `/opt/passengers-backend` контейнер собирается из директории `app/`, а не `backend/app/`;
- файл обновлён не в том пути, и билд уходит со старым кодом.

Проверка:

```bash
ssh alis@207.180.213.225 'sed -n "1,80p" /opt/passengers-backend/Dockerfile'
ssh alis@207.180.213.225 'ls -la /opt/passengers-backend/app /opt/passengers-backend/backend/app'
```

Решение:

```bash
scp backend/app/main.py alis@207.180.213.225:/tmp/passengers-main.py
ssh alis@207.180.213.225 'sudo install -m 644 -o alis -g alis /tmp/passengers-main.py /opt/passengers-backend/app/main.py && rm -f /tmp/passengers-main.py && cd /opt/passengers-backend && sudo docker compose -f compose.yaml -f compose.server.yaml up -d --build api'
```

## 20) `500` на `/admin/fleet/notifications` или `/admin/fleet/notify-center` после выноса в модуль

Симптом:

- route возвращает `500 Internal Server Error`;
- в логах `fastapi.exceptions.ResponseValidationError` и текст вида `Input should be a valid string`;
- в `input` видно `coroutine object render_admin_fleet_...`.

Причина:

- helper в модульном файле объявлен как `async def`, а thin-wrapper в `main.py` возвращает его без `await`.

Решение:

- для статических HTML-страниц объявлять helper как обычный `def ... -> str`;
- либо явно делать `return await ...` в wrapper (если async действительно нужен).

Проверка:

```bash
python3 -m py_compile backend/app/main.py backend/app/admin_fleet_notifications_page.py backend/app/admin_fleet_notify_center_page.py
ssh alis@207.180.213.225 'sudo docker logs --tail 120 passengers-backend-api-1 | rg -n "ResponseValidationError|coroutine object|notify-center|notifications"'
ssh alis@207.180.213.225 'curl -k -u "admin:<BASIC_AUTH_PASS>" -s -o /dev/null -w "%{http_code}\n" https://127.0.0.1:8443/admin/fleet/notifications'
ssh alis@207.180.213.225 'curl -k -u "admin:<BASIC_AUTH_PASS>" -s -o /dev/null -w "%{http_code}\n" https://127.0.0.1:8443/admin/fleet/notify-center'
```
